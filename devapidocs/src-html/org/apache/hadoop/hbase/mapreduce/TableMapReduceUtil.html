<!DOCTYPE HTML>
<html lang="en">
<head>
<!-- Generated by javadoc (17) -->
<title>Source code</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="source: package: org.apache.hadoop.hbase.mapreduce, class: TableMapReduceUtil">
<meta name="generator" content="javadoc/SourceToHTMLConverter">
<link rel="stylesheet" type="text/css" href="../../../../../../stylesheet.css" title="Style">
</head>
<body class="source-page">
<main role="main">
<div class="source-container">
<pre><span class="source-line-no">001</span><span id="line-1">/*</span>
<span class="source-line-no">002</span><span id="line-2"> * Licensed to the Apache Software Foundation (ASF) under one</span>
<span class="source-line-no">003</span><span id="line-3"> * or more contributor license agreements.  See the NOTICE file</span>
<span class="source-line-no">004</span><span id="line-4"> * distributed with this work for additional information</span>
<span class="source-line-no">005</span><span id="line-5"> * regarding copyright ownership.  The ASF licenses this file</span>
<span class="source-line-no">006</span><span id="line-6"> * to you under the Apache License, Version 2.0 (the</span>
<span class="source-line-no">007</span><span id="line-7"> * "License"); you may not use this file except in compliance</span>
<span class="source-line-no">008</span><span id="line-8"> * with the License.  You may obtain a copy of the License at</span>
<span class="source-line-no">009</span><span id="line-9"> *</span>
<span class="source-line-no">010</span><span id="line-10"> *     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="source-line-no">011</span><span id="line-11"> *</span>
<span class="source-line-no">012</span><span id="line-12"> * Unless required by applicable law or agreed to in writing, software</span>
<span class="source-line-no">013</span><span id="line-13"> * distributed under the License is distributed on an "AS IS" BASIS,</span>
<span class="source-line-no">014</span><span id="line-14"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="source-line-no">015</span><span id="line-15"> * See the License for the specific language governing permissions and</span>
<span class="source-line-no">016</span><span id="line-16"> * limitations under the License.</span>
<span class="source-line-no">017</span><span id="line-17"> */</span>
<span class="source-line-no">018</span><span id="line-18">package org.apache.hadoop.hbase.mapreduce;</span>
<span class="source-line-no">019</span><span id="line-19"></span>
<span class="source-line-no">020</span><span id="line-20">import com.codahale.metrics.MetricRegistry;</span>
<span class="source-line-no">021</span><span id="line-21">import java.io.File;</span>
<span class="source-line-no">022</span><span id="line-22">import java.io.IOException;</span>
<span class="source-line-no">023</span><span id="line-23">import java.net.URI;</span>
<span class="source-line-no">024</span><span id="line-24">import java.net.URISyntaxException;</span>
<span class="source-line-no">025</span><span id="line-25">import java.net.URL;</span>
<span class="source-line-no">026</span><span id="line-26">import java.net.URLDecoder;</span>
<span class="source-line-no">027</span><span id="line-27">import java.util.ArrayList;</span>
<span class="source-line-no">028</span><span id="line-28">import java.util.Base64;</span>
<span class="source-line-no">029</span><span id="line-29">import java.util.Collection;</span>
<span class="source-line-no">030</span><span id="line-30">import java.util.Enumeration;</span>
<span class="source-line-no">031</span><span id="line-31">import java.util.HashMap;</span>
<span class="source-line-no">032</span><span id="line-32">import java.util.HashSet;</span>
<span class="source-line-no">033</span><span id="line-33">import java.util.List;</span>
<span class="source-line-no">034</span><span id="line-34">import java.util.Map;</span>
<span class="source-line-no">035</span><span id="line-35">import java.util.Set;</span>
<span class="source-line-no">036</span><span id="line-36">import java.util.stream.Collectors;</span>
<span class="source-line-no">037</span><span id="line-37">import java.util.zip.ZipEntry;</span>
<span class="source-line-no">038</span><span id="line-38">import java.util.zip.ZipFile;</span>
<span class="source-line-no">039</span><span id="line-39">import org.apache.commons.lang3.StringUtils;</span>
<span class="source-line-no">040</span><span id="line-40">import org.apache.hadoop.conf.Configuration;</span>
<span class="source-line-no">041</span><span id="line-41">import org.apache.hadoop.fs.FileSystem;</span>
<span class="source-line-no">042</span><span id="line-42">import org.apache.hadoop.fs.Path;</span>
<span class="source-line-no">043</span><span id="line-43">import org.apache.hadoop.hbase.HBaseConfiguration;</span>
<span class="source-line-no">044</span><span id="line-44">import org.apache.hadoop.hbase.HConstants;</span>
<span class="source-line-no">045</span><span id="line-45">import org.apache.hadoop.hbase.TableName;</span>
<span class="source-line-no">046</span><span id="line-46">import org.apache.hadoop.hbase.client.Connection;</span>
<span class="source-line-no">047</span><span id="line-47">import org.apache.hadoop.hbase.client.ConnectionFactory;</span>
<span class="source-line-no">048</span><span id="line-48">import org.apache.hadoop.hbase.client.ConnectionRegistryFactory;</span>
<span class="source-line-no">049</span><span id="line-49">import org.apache.hadoop.hbase.client.Put;</span>
<span class="source-line-no">050</span><span id="line-50">import org.apache.hadoop.hbase.client.RegionLocator;</span>
<span class="source-line-no">051</span><span id="line-51">import org.apache.hadoop.hbase.client.Scan;</span>
<span class="source-line-no">052</span><span id="line-52">import org.apache.hadoop.hbase.io.ImmutableBytesWritable;</span>
<span class="source-line-no">053</span><span id="line-53">import org.apache.hadoop.hbase.security.User;</span>
<span class="source-line-no">054</span><span id="line-54">import org.apache.hadoop.hbase.security.UserProvider;</span>
<span class="source-line-no">055</span><span id="line-55">import org.apache.hadoop.hbase.security.token.TokenUtil;</span>
<span class="source-line-no">056</span><span id="line-56">import org.apache.hadoop.hbase.util.Bytes;</span>
<span class="source-line-no">057</span><span id="line-57">import org.apache.hadoop.hbase.util.IOExceptionRunnable;</span>
<span class="source-line-no">058</span><span id="line-58">import org.apache.hadoop.hbase.util.IOExceptionSupplier;</span>
<span class="source-line-no">059</span><span id="line-59">import org.apache.hadoop.hbase.util.RegionSplitter;</span>
<span class="source-line-no">060</span><span id="line-60">import org.apache.hadoop.hbase.zookeeper.ZKConfig;</span>
<span class="source-line-no">061</span><span id="line-61">import org.apache.hadoop.io.Writable;</span>
<span class="source-line-no">062</span><span id="line-62">import org.apache.hadoop.mapreduce.InputFormat;</span>
<span class="source-line-no">063</span><span id="line-63">import org.apache.hadoop.mapreduce.Job;</span>
<span class="source-line-no">064</span><span id="line-64">import org.apache.yetus.audience.InterfaceAudience;</span>
<span class="source-line-no">065</span><span id="line-65">import org.slf4j.Logger;</span>
<span class="source-line-no">066</span><span id="line-66">import org.slf4j.LoggerFactory;</span>
<span class="source-line-no">067</span><span id="line-67"></span>
<span class="source-line-no">068</span><span id="line-68">import org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil;</span>
<span class="source-line-no">069</span><span id="line-69">import org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos;</span>
<span class="source-line-no">070</span><span id="line-70"></span>
<span class="source-line-no">071</span><span id="line-71">/**</span>
<span class="source-line-no">072</span><span id="line-72"> * Utility for {@link TableMapper} and {@link TableReducer}</span>
<span class="source-line-no">073</span><span id="line-73"> */</span>
<span class="source-line-no">074</span><span id="line-74">@SuppressWarnings({ "rawtypes", "unchecked" })</span>
<span class="source-line-no">075</span><span id="line-75">@InterfaceAudience.Public</span>
<span class="source-line-no">076</span><span id="line-76">public class TableMapReduceUtil {</span>
<span class="source-line-no">077</span><span id="line-77">  private static final Logger LOG = LoggerFactory.getLogger(TableMapReduceUtil.class);</span>
<span class="source-line-no">078</span><span id="line-78">  public static final String TABLE_INPUT_CLASS_KEY = "hbase.table.input.class";</span>
<span class="source-line-no">079</span><span id="line-79"></span>
<span class="source-line-no">080</span><span id="line-80">  /**</span>
<span class="source-line-no">081</span><span id="line-81">   * Use this before submitting a TableMap job. It will appropriately set up the job.</span>
<span class="source-line-no">082</span><span id="line-82">   * @param table            The table name to read from.</span>
<span class="source-line-no">083</span><span id="line-83">   * @param scan             The scan instance with the columns, time range etc.</span>
<span class="source-line-no">084</span><span id="line-84">   * @param mapper           The mapper class to use.</span>
<span class="source-line-no">085</span><span id="line-85">   * @param outputKeyClass   The class of the output key.</span>
<span class="source-line-no">086</span><span id="line-86">   * @param outputValueClass The class of the output value.</span>
<span class="source-line-no">087</span><span id="line-87">   * @param job              The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">088</span><span id="line-88">   *                         necessary HBase configuration.</span>
<span class="source-line-no">089</span><span id="line-89">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">090</span><span id="line-90">   */</span>
<span class="source-line-no">091</span><span id="line-91">  public static void initTableMapperJob(String table, Scan scan,</span>
<span class="source-line-no">092</span><span id="line-92">    Class&lt;? extends TableMapper&gt; mapper, Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass,</span>
<span class="source-line-no">093</span><span id="line-93">    Job job) throws IOException {</span>
<span class="source-line-no">094</span><span id="line-94">    initTableMapperJob(table, scan, mapper, outputKeyClass, outputValueClass, job, true);</span>
<span class="source-line-no">095</span><span id="line-95">  }</span>
<span class="source-line-no">096</span><span id="line-96"></span>
<span class="source-line-no">097</span><span id="line-97">  /**</span>
<span class="source-line-no">098</span><span id="line-98">   * Use this before submitting a TableMap job. It will appropriately set up the job.</span>
<span class="source-line-no">099</span><span id="line-99">   * @param table            The table name to read from.</span>
<span class="source-line-no">100</span><span id="line-100">   * @param scan             The scan instance with the columns, time range etc.</span>
<span class="source-line-no">101</span><span id="line-101">   * @param mapper           The mapper class to use.</span>
<span class="source-line-no">102</span><span id="line-102">   * @param outputKeyClass   The class of the output key.</span>
<span class="source-line-no">103</span><span id="line-103">   * @param outputValueClass The class of the output value.</span>
<span class="source-line-no">104</span><span id="line-104">   * @param job              The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">105</span><span id="line-105">   *                         necessary HBase configuration.</span>
<span class="source-line-no">106</span><span id="line-106">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">107</span><span id="line-107">   */</span>
<span class="source-line-no">108</span><span id="line-108">  public static void initTableMapperJob(TableName table, Scan scan,</span>
<span class="source-line-no">109</span><span id="line-109">    Class&lt;? extends TableMapper&gt; mapper, Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass,</span>
<span class="source-line-no">110</span><span id="line-110">    Job job) throws IOException {</span>
<span class="source-line-no">111</span><span id="line-111">    initTableMapperJob(table.getNameAsString(), scan, mapper, outputKeyClass, outputValueClass, job,</span>
<span class="source-line-no">112</span><span id="line-112">      true);</span>
<span class="source-line-no">113</span><span id="line-113">  }</span>
<span class="source-line-no">114</span><span id="line-114"></span>
<span class="source-line-no">115</span><span id="line-115">  /**</span>
<span class="source-line-no">116</span><span id="line-116">   * Use this before submitting a TableMap job. It will appropriately set up the job.</span>
<span class="source-line-no">117</span><span id="line-117">   * @param table            Binary representation of the table name to read from.</span>
<span class="source-line-no">118</span><span id="line-118">   * @param scan             The scan instance with the columns, time range etc.</span>
<span class="source-line-no">119</span><span id="line-119">   * @param mapper           The mapper class to use.</span>
<span class="source-line-no">120</span><span id="line-120">   * @param outputKeyClass   The class of the output key.</span>
<span class="source-line-no">121</span><span id="line-121">   * @param outputValueClass The class of the output value.</span>
<span class="source-line-no">122</span><span id="line-122">   * @param job              The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">123</span><span id="line-123">   *                         necessary HBase configuration.</span>
<span class="source-line-no">124</span><span id="line-124">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">125</span><span id="line-125">   */</span>
<span class="source-line-no">126</span><span id="line-126">  public static void initTableMapperJob(byte[] table, Scan scan,</span>
<span class="source-line-no">127</span><span id="line-127">    Class&lt;? extends TableMapper&gt; mapper, Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass,</span>
<span class="source-line-no">128</span><span id="line-128">    Job job) throws IOException {</span>
<span class="source-line-no">129</span><span id="line-129">    initTableMapperJob(Bytes.toString(table), scan, mapper, outputKeyClass, outputValueClass, job,</span>
<span class="source-line-no">130</span><span id="line-130">      true);</span>
<span class="source-line-no">131</span><span id="line-131">  }</span>
<span class="source-line-no">132</span><span id="line-132"></span>
<span class="source-line-no">133</span><span id="line-133">  /**</span>
<span class="source-line-no">134</span><span id="line-134">   * Use this before submitting a TableMap job. It will appropriately set up the job.</span>
<span class="source-line-no">135</span><span id="line-135">   * @param table             The table name to read from.</span>
<span class="source-line-no">136</span><span id="line-136">   * @param scan              The scan instance with the columns, time range etc.</span>
<span class="source-line-no">137</span><span id="line-137">   * @param mapper            The mapper class to use.</span>
<span class="source-line-no">138</span><span id="line-138">   * @param outputKeyClass    The class of the output key.</span>
<span class="source-line-no">139</span><span id="line-139">   * @param outputValueClass  The class of the output value.</span>
<span class="source-line-no">140</span><span id="line-140">   * @param job               The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">141</span><span id="line-141">   *                          necessary HBase configuration.</span>
<span class="source-line-no">142</span><span id="line-142">   * @param addDependencyJars upload HBase jars and jars for any of the configured job classes via</span>
<span class="source-line-no">143</span><span id="line-143">   *                          the distributed cache (tmpjars).</span>
<span class="source-line-no">144</span><span id="line-144">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">145</span><span id="line-145">   */</span>
<span class="source-line-no">146</span><span id="line-146">  public static void initTableMapperJob(String table, Scan scan,</span>
<span class="source-line-no">147</span><span id="line-147">    Class&lt;? extends TableMapper&gt; mapper, Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass,</span>
<span class="source-line-no">148</span><span id="line-148">    Job job, boolean addDependencyJars, Class&lt;? extends InputFormat&gt; inputFormatClass)</span>
<span class="source-line-no">149</span><span id="line-149">    throws IOException {</span>
<span class="source-line-no">150</span><span id="line-150">    initTableMapperJob(table, scan, mapper, outputKeyClass, outputValueClass, job,</span>
<span class="source-line-no">151</span><span id="line-151">      addDependencyJars, true, inputFormatClass);</span>
<span class="source-line-no">152</span><span id="line-152">  }</span>
<span class="source-line-no">153</span><span id="line-153"></span>
<span class="source-line-no">154</span><span id="line-154">  /**</span>
<span class="source-line-no">155</span><span id="line-155">   * Use this before submitting a TableMap job. It will appropriately set up the job.</span>
<span class="source-line-no">156</span><span id="line-156">   * @param table             The table name to read from.</span>
<span class="source-line-no">157</span><span id="line-157">   * @param scan              The scan instance with the columns, time range etc.</span>
<span class="source-line-no">158</span><span id="line-158">   * @param mapper            The mapper class to use.</span>
<span class="source-line-no">159</span><span id="line-159">   * @param outputKeyClass    The class of the output key.</span>
<span class="source-line-no">160</span><span id="line-160">   * @param outputValueClass  The class of the output value.</span>
<span class="source-line-no">161</span><span id="line-161">   * @param job               The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">162</span><span id="line-162">   *                          necessary HBase configuration.</span>
<span class="source-line-no">163</span><span id="line-163">   * @param addDependencyJars upload HBase jars and jars for any of the configured job classes via</span>
<span class="source-line-no">164</span><span id="line-164">   *                          the distributed cache (tmpjars).</span>
<span class="source-line-no">165</span><span id="line-165">   * @param initCredentials   whether to initialize hbase auth credentials for the job</span>
<span class="source-line-no">166</span><span id="line-166">   * @param inputFormatClass  the input format</span>
<span class="source-line-no">167</span><span id="line-167">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">168</span><span id="line-168">   */</span>
<span class="source-line-no">169</span><span id="line-169">  public static void initTableMapperJob(String table, Scan scan,</span>
<span class="source-line-no">170</span><span id="line-170">    Class&lt;? extends TableMapper&gt; mapper, Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass,</span>
<span class="source-line-no">171</span><span id="line-171">    Job job, boolean addDependencyJars, boolean initCredentials,</span>
<span class="source-line-no">172</span><span id="line-172">    Class&lt;? extends InputFormat&gt; inputFormatClass) throws IOException {</span>
<span class="source-line-no">173</span><span id="line-173">    job.setInputFormatClass(inputFormatClass);</span>
<span class="source-line-no">174</span><span id="line-174">    if (outputValueClass != null) job.setMapOutputValueClass(outputValueClass);</span>
<span class="source-line-no">175</span><span id="line-175">    if (outputKeyClass != null) job.setMapOutputKeyClass(outputKeyClass);</span>
<span class="source-line-no">176</span><span id="line-176">    job.setMapperClass(mapper);</span>
<span class="source-line-no">177</span><span id="line-177">    if (Put.class.equals(outputValueClass)) {</span>
<span class="source-line-no">178</span><span id="line-178">      job.setCombinerClass(PutCombiner.class);</span>
<span class="source-line-no">179</span><span id="line-179">    }</span>
<span class="source-line-no">180</span><span id="line-180">    Configuration conf = job.getConfiguration();</span>
<span class="source-line-no">181</span><span id="line-181">    HBaseConfiguration.merge(conf, HBaseConfiguration.create(conf));</span>
<span class="source-line-no">182</span><span id="line-182">    conf.set(TableInputFormat.INPUT_TABLE, table);</span>
<span class="source-line-no">183</span><span id="line-183">    conf.set(TableInputFormat.SCAN, convertScanToString(scan));</span>
<span class="source-line-no">184</span><span id="line-184">    conf.setStrings("io.serializations", conf.get("io.serializations"),</span>
<span class="source-line-no">185</span><span id="line-185">      MutationSerialization.class.getName(), ResultSerialization.class.getName(),</span>
<span class="source-line-no">186</span><span id="line-186">      CellSerialization.class.getName());</span>
<span class="source-line-no">187</span><span id="line-187">    if (addDependencyJars) {</span>
<span class="source-line-no">188</span><span id="line-188">      addDependencyJars(job);</span>
<span class="source-line-no">189</span><span id="line-189">    }</span>
<span class="source-line-no">190</span><span id="line-190">    if (initCredentials) {</span>
<span class="source-line-no">191</span><span id="line-191">      initCredentials(job);</span>
<span class="source-line-no">192</span><span id="line-192">    }</span>
<span class="source-line-no">193</span><span id="line-193">  }</span>
<span class="source-line-no">194</span><span id="line-194"></span>
<span class="source-line-no">195</span><span id="line-195">  /**</span>
<span class="source-line-no">196</span><span id="line-196">   * Use this before submitting a TableMap job. It will appropriately set up the job.</span>
<span class="source-line-no">197</span><span id="line-197">   * @param table             Binary representation of the table name to read from.</span>
<span class="source-line-no">198</span><span id="line-198">   * @param scan              The scan instance with the columns, time range etc.</span>
<span class="source-line-no">199</span><span id="line-199">   * @param mapper            The mapper class to use.</span>
<span class="source-line-no">200</span><span id="line-200">   * @param outputKeyClass    The class of the output key.</span>
<span class="source-line-no">201</span><span id="line-201">   * @param outputValueClass  The class of the output value.</span>
<span class="source-line-no">202</span><span id="line-202">   * @param job               The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">203</span><span id="line-203">   *                          necessary HBase configuration.</span>
<span class="source-line-no">204</span><span id="line-204">   * @param addDependencyJars upload HBase jars and jars for any of the configured job classes via</span>
<span class="source-line-no">205</span><span id="line-205">   *                          the distributed cache (tmpjars).</span>
<span class="source-line-no">206</span><span id="line-206">   * @param inputFormatClass  The class of the input format</span>
<span class="source-line-no">207</span><span id="line-207">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">208</span><span id="line-208">   */</span>
<span class="source-line-no">209</span><span id="line-209">  public static void initTableMapperJob(byte[] table, Scan scan,</span>
<span class="source-line-no">210</span><span id="line-210">    Class&lt;? extends TableMapper&gt; mapper, Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass,</span>
<span class="source-line-no">211</span><span id="line-211">    Job job, boolean addDependencyJars, Class&lt;? extends InputFormat&gt; inputFormatClass)</span>
<span class="source-line-no">212</span><span id="line-212">    throws IOException {</span>
<span class="source-line-no">213</span><span id="line-213">    initTableMapperJob(Bytes.toString(table), scan, mapper, outputKeyClass, outputValueClass, job,</span>
<span class="source-line-no">214</span><span id="line-214">      addDependencyJars, inputFormatClass);</span>
<span class="source-line-no">215</span><span id="line-215">  }</span>
<span class="source-line-no">216</span><span id="line-216"></span>
<span class="source-line-no">217</span><span id="line-217">  /**</span>
<span class="source-line-no">218</span><span id="line-218">   * Use this before submitting a TableMap job. It will appropriately set up the job.</span>
<span class="source-line-no">219</span><span id="line-219">   * @param table             Binary representation of the table name to read from.</span>
<span class="source-line-no">220</span><span id="line-220">   * @param scan              The scan instance with the columns, time range etc.</span>
<span class="source-line-no">221</span><span id="line-221">   * @param mapper            The mapper class to use.</span>
<span class="source-line-no">222</span><span id="line-222">   * @param outputKeyClass    The class of the output key.</span>
<span class="source-line-no">223</span><span id="line-223">   * @param outputValueClass  The class of the output value.</span>
<span class="source-line-no">224</span><span id="line-224">   * @param job               The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">225</span><span id="line-225">   *                          necessary HBase configuration.</span>
<span class="source-line-no">226</span><span id="line-226">   * @param addDependencyJars upload HBase jars and jars for any of the configured job classes via</span>
<span class="source-line-no">227</span><span id="line-227">   *                          the distributed cache (tmpjars).</span>
<span class="source-line-no">228</span><span id="line-228">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">229</span><span id="line-229">   */</span>
<span class="source-line-no">230</span><span id="line-230">  public static void initTableMapperJob(byte[] table, Scan scan,</span>
<span class="source-line-no">231</span><span id="line-231">    Class&lt;? extends TableMapper&gt; mapper, Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass,</span>
<span class="source-line-no">232</span><span id="line-232">    Job job, boolean addDependencyJars) throws IOException {</span>
<span class="source-line-no">233</span><span id="line-233">    initTableMapperJob(Bytes.toString(table), scan, mapper, outputKeyClass, outputValueClass, job,</span>
<span class="source-line-no">234</span><span id="line-234">      addDependencyJars, getConfiguredInputFormat(job));</span>
<span class="source-line-no">235</span><span id="line-235">  }</span>
<span class="source-line-no">236</span><span id="line-236"></span>
<span class="source-line-no">237</span><span id="line-237">  /**</span>
<span class="source-line-no">238</span><span id="line-238">   * @return {@link TableInputFormat} .class unless Configuration has something else at</span>
<span class="source-line-no">239</span><span id="line-239">   *         {@link #TABLE_INPUT_CLASS_KEY}.</span>
<span class="source-line-no">240</span><span id="line-240">   */</span>
<span class="source-line-no">241</span><span id="line-241">  private static Class&lt;? extends InputFormat&gt; getConfiguredInputFormat(Job job) {</span>
<span class="source-line-no">242</span><span id="line-242">    return (Class&lt;? extends InputFormat&gt;) job.getConfiguration().getClass(TABLE_INPUT_CLASS_KEY,</span>
<span class="source-line-no">243</span><span id="line-243">      TableInputFormat.class);</span>
<span class="source-line-no">244</span><span id="line-244">  }</span>
<span class="source-line-no">245</span><span id="line-245"></span>
<span class="source-line-no">246</span><span id="line-246">  /**</span>
<span class="source-line-no">247</span><span id="line-247">   * Use this before submitting a TableMap job. It will appropriately set up the job.</span>
<span class="source-line-no">248</span><span id="line-248">   * @param table             The table name to read from.</span>
<span class="source-line-no">249</span><span id="line-249">   * @param scan              The scan instance with the columns, time range etc.</span>
<span class="source-line-no">250</span><span id="line-250">   * @param mapper            The mapper class to use.</span>
<span class="source-line-no">251</span><span id="line-251">   * @param outputKeyClass    The class of the output key.</span>
<span class="source-line-no">252</span><span id="line-252">   * @param outputValueClass  The class of the output value.</span>
<span class="source-line-no">253</span><span id="line-253">   * @param job               The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">254</span><span id="line-254">   *                          necessary HBase configuration.</span>
<span class="source-line-no">255</span><span id="line-255">   * @param addDependencyJars upload HBase jars and jars for any of the configured job classes via</span>
<span class="source-line-no">256</span><span id="line-256">   *                          the distributed cache (tmpjars).</span>
<span class="source-line-no">257</span><span id="line-257">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">258</span><span id="line-258">   */</span>
<span class="source-line-no">259</span><span id="line-259">  public static void initTableMapperJob(String table, Scan scan,</span>
<span class="source-line-no">260</span><span id="line-260">    Class&lt;? extends TableMapper&gt; mapper, Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass,</span>
<span class="source-line-no">261</span><span id="line-261">    Job job, boolean addDependencyJars) throws IOException {</span>
<span class="source-line-no">262</span><span id="line-262">    initTableMapperJob(table, scan, mapper, outputKeyClass, outputValueClass, job,</span>
<span class="source-line-no">263</span><span id="line-263">      addDependencyJars, getConfiguredInputFormat(job));</span>
<span class="source-line-no">264</span><span id="line-264">  }</span>
<span class="source-line-no">265</span><span id="line-265"></span>
<span class="source-line-no">266</span><span id="line-266">  /**</span>
<span class="source-line-no">267</span><span id="line-267">   * Enable a basic on-heap cache for these jobs. Any BlockCache implementation based on direct</span>
<span class="source-line-no">268</span><span id="line-268">   * memory will likely cause the map tasks to OOM when opening the region. This is done here</span>
<span class="source-line-no">269</span><span id="line-269">   * instead of in TableSnapshotRegionRecordReader in case an advanced user wants to override this</span>
<span class="source-line-no">270</span><span id="line-270">   * behavior in their job.</span>
<span class="source-line-no">271</span><span id="line-271">   */</span>
<span class="source-line-no">272</span><span id="line-272">  public static void resetCacheConfig(Configuration conf) {</span>
<span class="source-line-no">273</span><span id="line-273">    conf.setFloat(HConstants.HFILE_BLOCK_CACHE_SIZE_KEY, HConstants.HFILE_BLOCK_CACHE_SIZE_DEFAULT);</span>
<span class="source-line-no">274</span><span id="line-274">    conf.setFloat(HConstants.BUCKET_CACHE_SIZE_KEY, 0f);</span>
<span class="source-line-no">275</span><span id="line-275">    conf.unset(HConstants.BUCKET_CACHE_IOENGINE_KEY);</span>
<span class="source-line-no">276</span><span id="line-276">  }</span>
<span class="source-line-no">277</span><span id="line-277"></span>
<span class="source-line-no">278</span><span id="line-278">  /**</span>
<span class="source-line-no">279</span><span id="line-279">   * Sets up the job for reading from one or more table snapshots, with one or more scans per</span>
<span class="source-line-no">280</span><span id="line-280">   * snapshot. It bypasses hbase servers and read directly from snapshot files.</span>
<span class="source-line-no">281</span><span id="line-281">   * @param snapshotScans     map of snapshot name to scans on that snapshot.</span>
<span class="source-line-no">282</span><span id="line-282">   * @param mapper            The mapper class to use.</span>
<span class="source-line-no">283</span><span id="line-283">   * @param outputKeyClass    The class of the output key.</span>
<span class="source-line-no">284</span><span id="line-284">   * @param outputValueClass  The class of the output value.</span>
<span class="source-line-no">285</span><span id="line-285">   * @param job               The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">286</span><span id="line-286">   *                          necessary HBase configuration.</span>
<span class="source-line-no">287</span><span id="line-287">   * @param addDependencyJars upload HBase jars and jars for any of the configured job classes via</span>
<span class="source-line-no">288</span><span id="line-288">   *                          the distributed cache (tmpjars).</span>
<span class="source-line-no">289</span><span id="line-289">   */</span>
<span class="source-line-no">290</span><span id="line-290">  public static void initMultiTableSnapshotMapperJob(Map&lt;String, Collection&lt;Scan&gt;&gt; snapshotScans,</span>
<span class="source-line-no">291</span><span id="line-291">    Class&lt;? extends TableMapper&gt; mapper, Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass,</span>
<span class="source-line-no">292</span><span id="line-292">    Job job, boolean addDependencyJars, Path tmpRestoreDir) throws IOException {</span>
<span class="source-line-no">293</span><span id="line-293">    MultiTableSnapshotInputFormat.setInput(job.getConfiguration(), snapshotScans, tmpRestoreDir);</span>
<span class="source-line-no">294</span><span id="line-294"></span>
<span class="source-line-no">295</span><span id="line-295">    job.setInputFormatClass(MultiTableSnapshotInputFormat.class);</span>
<span class="source-line-no">296</span><span id="line-296">    if (outputValueClass != null) {</span>
<span class="source-line-no">297</span><span id="line-297">      job.setMapOutputValueClass(outputValueClass);</span>
<span class="source-line-no">298</span><span id="line-298">    }</span>
<span class="source-line-no">299</span><span id="line-299">    if (outputKeyClass != null) {</span>
<span class="source-line-no">300</span><span id="line-300">      job.setMapOutputKeyClass(outputKeyClass);</span>
<span class="source-line-no">301</span><span id="line-301">    }</span>
<span class="source-line-no">302</span><span id="line-302">    job.setMapperClass(mapper);</span>
<span class="source-line-no">303</span><span id="line-303">    Configuration conf = job.getConfiguration();</span>
<span class="source-line-no">304</span><span id="line-304">    HBaseConfiguration.merge(conf, HBaseConfiguration.create(conf));</span>
<span class="source-line-no">305</span><span id="line-305"></span>
<span class="source-line-no">306</span><span id="line-306">    if (addDependencyJars) {</span>
<span class="source-line-no">307</span><span id="line-307">      addDependencyJars(job);</span>
<span class="source-line-no">308</span><span id="line-308">      addDependencyJarsForClasses(job.getConfiguration(), MetricRegistry.class);</span>
<span class="source-line-no">309</span><span id="line-309">    }</span>
<span class="source-line-no">310</span><span id="line-310"></span>
<span class="source-line-no">311</span><span id="line-311">    resetCacheConfig(job.getConfiguration());</span>
<span class="source-line-no">312</span><span id="line-312">  }</span>
<span class="source-line-no">313</span><span id="line-313"></span>
<span class="source-line-no">314</span><span id="line-314">  /**</span>
<span class="source-line-no">315</span><span id="line-315">   * Sets up the job for reading from a table snapshot. It bypasses hbase servers and read directly</span>
<span class="source-line-no">316</span><span id="line-316">   * from snapshot files.</span>
<span class="source-line-no">317</span><span id="line-317">   * @param snapshotName      The name of the snapshot (of a table) to read from.</span>
<span class="source-line-no">318</span><span id="line-318">   * @param scan              The scan instance with the columns, time range etc.</span>
<span class="source-line-no">319</span><span id="line-319">   * @param mapper            The mapper class to use.</span>
<span class="source-line-no">320</span><span id="line-320">   * @param outputKeyClass    The class of the output key.</span>
<span class="source-line-no">321</span><span id="line-321">   * @param outputValueClass  The class of the output value.</span>
<span class="source-line-no">322</span><span id="line-322">   * @param job               The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">323</span><span id="line-323">   *                          necessary HBase configuration.</span>
<span class="source-line-no">324</span><span id="line-324">   * @param addDependencyJars upload HBase jars and jars for any of the configured job classes via</span>
<span class="source-line-no">325</span><span id="line-325">   *                          the distributed cache (tmpjars).</span>
<span class="source-line-no">326</span><span id="line-326">   * @param tmpRestoreDir     a temporary directory to copy the snapshot files into. Current user</span>
<span class="source-line-no">327</span><span id="line-327">   *                          should have write permissions to this directory, and this should not</span>
<span class="source-line-no">328</span><span id="line-328">   *                          be a subdirectory of rootdir. After the job is finished, restore</span>
<span class="source-line-no">329</span><span id="line-329">   *                          directory can be deleted.</span>
<span class="source-line-no">330</span><span id="line-330">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">331</span><span id="line-331">   * @see TableSnapshotInputFormat</span>
<span class="source-line-no">332</span><span id="line-332">   */</span>
<span class="source-line-no">333</span><span id="line-333">  public static void initTableSnapshotMapperJob(String snapshotName, Scan scan,</span>
<span class="source-line-no">334</span><span id="line-334">    Class&lt;? extends TableMapper&gt; mapper, Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass,</span>
<span class="source-line-no">335</span><span id="line-335">    Job job, boolean addDependencyJars, Path tmpRestoreDir) throws IOException {</span>
<span class="source-line-no">336</span><span id="line-336">    TableSnapshotInputFormat.setInput(job, snapshotName, tmpRestoreDir);</span>
<span class="source-line-no">337</span><span id="line-337">    initTableMapperJob(snapshotName, scan, mapper, outputKeyClass, outputValueClass, job,</span>
<span class="source-line-no">338</span><span id="line-338">      addDependencyJars, false, TableSnapshotInputFormat.class);</span>
<span class="source-line-no">339</span><span id="line-339">    resetCacheConfig(job.getConfiguration());</span>
<span class="source-line-no">340</span><span id="line-340">  }</span>
<span class="source-line-no">341</span><span id="line-341"></span>
<span class="source-line-no">342</span><span id="line-342">  /**</span>
<span class="source-line-no">343</span><span id="line-343">   * Sets up the job for reading from a table snapshot. It bypasses hbase servers and read directly</span>
<span class="source-line-no">344</span><span id="line-344">   * from snapshot files.</span>
<span class="source-line-no">345</span><span id="line-345">   * @param snapshotName       The name of the snapshot (of a table) to read from.</span>
<span class="source-line-no">346</span><span id="line-346">   * @param scan               The scan instance with the columns, time range etc.</span>
<span class="source-line-no">347</span><span id="line-347">   * @param mapper             The mapper class to use.</span>
<span class="source-line-no">348</span><span id="line-348">   * @param outputKeyClass     The class of the output key.</span>
<span class="source-line-no">349</span><span id="line-349">   * @param outputValueClass   The class of the output value.</span>
<span class="source-line-no">350</span><span id="line-350">   * @param job                The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">351</span><span id="line-351">   *                           necessary HBase configuration.</span>
<span class="source-line-no">352</span><span id="line-352">   * @param addDependencyJars  upload HBase jars and jars for any of the configured job classes via</span>
<span class="source-line-no">353</span><span id="line-353">   *                           the distributed cache (tmpjars).</span>
<span class="source-line-no">354</span><span id="line-354">   * @param tmpRestoreDir      a temporary directory to copy the snapshot files into. Current user</span>
<span class="source-line-no">355</span><span id="line-355">   *                           should have write permissions to this directory, and this should not</span>
<span class="source-line-no">356</span><span id="line-356">   *                           be a subdirectory of rootdir. After the job is finished, restore</span>
<span class="source-line-no">357</span><span id="line-357">   *                           directory can be deleted.</span>
<span class="source-line-no">358</span><span id="line-358">   * @param splitAlgo          algorithm to split</span>
<span class="source-line-no">359</span><span id="line-359">   * @param numSplitsPerRegion how many input splits to generate per one region</span>
<span class="source-line-no">360</span><span id="line-360">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">361</span><span id="line-361">   * @see TableSnapshotInputFormat</span>
<span class="source-line-no">362</span><span id="line-362">   */</span>
<span class="source-line-no">363</span><span id="line-363">  public static void initTableSnapshotMapperJob(String snapshotName, Scan scan,</span>
<span class="source-line-no">364</span><span id="line-364">    Class&lt;? extends TableMapper&gt; mapper, Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass,</span>
<span class="source-line-no">365</span><span id="line-365">    Job job, boolean addDependencyJars, Path tmpRestoreDir, RegionSplitter.SplitAlgorithm splitAlgo,</span>
<span class="source-line-no">366</span><span id="line-366">    int numSplitsPerRegion) throws IOException {</span>
<span class="source-line-no">367</span><span id="line-367">    TableSnapshotInputFormat.setInput(job, snapshotName, tmpRestoreDir, splitAlgo,</span>
<span class="source-line-no">368</span><span id="line-368">      numSplitsPerRegion);</span>
<span class="source-line-no">369</span><span id="line-369">    initTableMapperJob(snapshotName, scan, mapper, outputKeyClass, outputValueClass, job,</span>
<span class="source-line-no">370</span><span id="line-370">      addDependencyJars, false, TableSnapshotInputFormat.class);</span>
<span class="source-line-no">371</span><span id="line-371">    resetCacheConfig(job.getConfiguration());</span>
<span class="source-line-no">372</span><span id="line-372">  }</span>
<span class="source-line-no">373</span><span id="line-373"></span>
<span class="source-line-no">374</span><span id="line-374">  /**</span>
<span class="source-line-no">375</span><span id="line-375">   * Use this before submitting a Multi TableMap job. It will appropriately set up the job.</span>
<span class="source-line-no">376</span><span id="line-376">   * @param scans            The list of {@link Scan} objects to read from.</span>
<span class="source-line-no">377</span><span id="line-377">   * @param mapper           The mapper class to use.</span>
<span class="source-line-no">378</span><span id="line-378">   * @param outputKeyClass   The class of the output key.</span>
<span class="source-line-no">379</span><span id="line-379">   * @param outputValueClass The class of the output value.</span>
<span class="source-line-no">380</span><span id="line-380">   * @param job              The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">381</span><span id="line-381">   *                         necessary HBase configuration.</span>
<span class="source-line-no">382</span><span id="line-382">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">383</span><span id="line-383">   */</span>
<span class="source-line-no">384</span><span id="line-384">  public static void initTableMapperJob(List&lt;Scan&gt; scans, Class&lt;? extends TableMapper&gt; mapper,</span>
<span class="source-line-no">385</span><span id="line-385">    Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass, Job job) throws IOException {</span>
<span class="source-line-no">386</span><span id="line-386">    initTableMapperJob(scans, mapper, outputKeyClass, outputValueClass, job, true);</span>
<span class="source-line-no">387</span><span id="line-387">  }</span>
<span class="source-line-no">388</span><span id="line-388"></span>
<span class="source-line-no">389</span><span id="line-389">  /**</span>
<span class="source-line-no">390</span><span id="line-390">   * Use this before submitting a Multi TableMap job. It will appropriately set up the job.</span>
<span class="source-line-no">391</span><span id="line-391">   * @param scans             The list of {@link Scan} objects to read from.</span>
<span class="source-line-no">392</span><span id="line-392">   * @param mapper            The mapper class to use.</span>
<span class="source-line-no">393</span><span id="line-393">   * @param outputKeyClass    The class of the output key.</span>
<span class="source-line-no">394</span><span id="line-394">   * @param outputValueClass  The class of the output value.</span>
<span class="source-line-no">395</span><span id="line-395">   * @param job               The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">396</span><span id="line-396">   *                          necessary HBase configuration.</span>
<span class="source-line-no">397</span><span id="line-397">   * @param addDependencyJars upload HBase jars and jars for any of the configured job classes via</span>
<span class="source-line-no">398</span><span id="line-398">   *                          the distributed cache (tmpjars).</span>
<span class="source-line-no">399</span><span id="line-399">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">400</span><span id="line-400">   */</span>
<span class="source-line-no">401</span><span id="line-401">  public static void initTableMapperJob(List&lt;Scan&gt; scans, Class&lt;? extends TableMapper&gt; mapper,</span>
<span class="source-line-no">402</span><span id="line-402">    Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass, Job job, boolean addDependencyJars)</span>
<span class="source-line-no">403</span><span id="line-403">    throws IOException {</span>
<span class="source-line-no">404</span><span id="line-404">    initTableMapperJob(scans, mapper, outputKeyClass, outputValueClass, job, addDependencyJars,</span>
<span class="source-line-no">405</span><span id="line-405">      true);</span>
<span class="source-line-no">406</span><span id="line-406">  }</span>
<span class="source-line-no">407</span><span id="line-407"></span>
<span class="source-line-no">408</span><span id="line-408">  /**</span>
<span class="source-line-no">409</span><span id="line-409">   * Use this before submitting a Multi TableMap job. It will appropriately set up the job.</span>
<span class="source-line-no">410</span><span id="line-410">   * @param scans             The list of {@link Scan} objects to read from.</span>
<span class="source-line-no">411</span><span id="line-411">   * @param mapper            The mapper class to use.</span>
<span class="source-line-no">412</span><span id="line-412">   * @param outputKeyClass    The class of the output key.</span>
<span class="source-line-no">413</span><span id="line-413">   * @param outputValueClass  The class of the output value.</span>
<span class="source-line-no">414</span><span id="line-414">   * @param job               The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">415</span><span id="line-415">   *                          necessary HBase configuration.</span>
<span class="source-line-no">416</span><span id="line-416">   * @param addDependencyJars upload HBase jars and jars for any of the configured job classes via</span>
<span class="source-line-no">417</span><span id="line-417">   *                          the distributed cache (tmpjars).</span>
<span class="source-line-no">418</span><span id="line-418">   * @param initCredentials   whether to initialize hbase auth credentials for the job</span>
<span class="source-line-no">419</span><span id="line-419">   * @throws IOException When setting up the details fails.</span>
<span class="source-line-no">420</span><span id="line-420">   */</span>
<span class="source-line-no">421</span><span id="line-421">  public static void initTableMapperJob(List&lt;Scan&gt; scans, Class&lt;? extends TableMapper&gt; mapper,</span>
<span class="source-line-no">422</span><span id="line-422">    Class&lt;?&gt; outputKeyClass, Class&lt;?&gt; outputValueClass, Job job, boolean addDependencyJars,</span>
<span class="source-line-no">423</span><span id="line-423">    boolean initCredentials) throws IOException {</span>
<span class="source-line-no">424</span><span id="line-424">    job.setInputFormatClass(MultiTableInputFormat.class);</span>
<span class="source-line-no">425</span><span id="line-425">    if (outputValueClass != null) {</span>
<span class="source-line-no">426</span><span id="line-426">      job.setMapOutputValueClass(outputValueClass);</span>
<span class="source-line-no">427</span><span id="line-427">    }</span>
<span class="source-line-no">428</span><span id="line-428">    if (outputKeyClass != null) {</span>
<span class="source-line-no">429</span><span id="line-429">      job.setMapOutputKeyClass(outputKeyClass);</span>
<span class="source-line-no">430</span><span id="line-430">    }</span>
<span class="source-line-no">431</span><span id="line-431">    job.setMapperClass(mapper);</span>
<span class="source-line-no">432</span><span id="line-432">    Configuration conf = job.getConfiguration();</span>
<span class="source-line-no">433</span><span id="line-433">    HBaseConfiguration.merge(conf, HBaseConfiguration.create(conf));</span>
<span class="source-line-no">434</span><span id="line-434">    List&lt;String&gt; scanStrings = new ArrayList&lt;&gt;();</span>
<span class="source-line-no">435</span><span id="line-435"></span>
<span class="source-line-no">436</span><span id="line-436">    for (Scan scan : scans) {</span>
<span class="source-line-no">437</span><span id="line-437">      scanStrings.add(convertScanToString(scan));</span>
<span class="source-line-no">438</span><span id="line-438">    }</span>
<span class="source-line-no">439</span><span id="line-439">    job.getConfiguration().setStrings(MultiTableInputFormat.SCANS,</span>
<span class="source-line-no">440</span><span id="line-440">      scanStrings.toArray(new String[scanStrings.size()]));</span>
<span class="source-line-no">441</span><span id="line-441"></span>
<span class="source-line-no">442</span><span id="line-442">    if (addDependencyJars) {</span>
<span class="source-line-no">443</span><span id="line-443">      addDependencyJars(job);</span>
<span class="source-line-no">444</span><span id="line-444">    }</span>
<span class="source-line-no">445</span><span id="line-445"></span>
<span class="source-line-no">446</span><span id="line-446">    if (initCredentials) {</span>
<span class="source-line-no">447</span><span id="line-447">      initCredentials(job);</span>
<span class="source-line-no">448</span><span id="line-448">    }</span>
<span class="source-line-no">449</span><span id="line-449">  }</span>
<span class="source-line-no">450</span><span id="line-450"></span>
<span class="source-line-no">451</span><span id="line-451">  private static void addTokenForJob(IOExceptionSupplier&lt;Connection&gt; connSupplier, User user,</span>
<span class="source-line-no">452</span><span id="line-452">    Job job) throws IOException, InterruptedException {</span>
<span class="source-line-no">453</span><span id="line-453">    try (Connection conn = connSupplier.get()) {</span>
<span class="source-line-no">454</span><span id="line-454">      TokenUtil.addTokenForJob(conn, user, job);</span>
<span class="source-line-no">455</span><span id="line-455">    }</span>
<span class="source-line-no">456</span><span id="line-456">  }</span>
<span class="source-line-no">457</span><span id="line-457"></span>
<span class="source-line-no">458</span><span id="line-458">  public static void initCredentials(Job job) throws IOException {</span>
<span class="source-line-no">459</span><span id="line-459">    UserProvider userProvider = UserProvider.instantiate(job.getConfiguration());</span>
<span class="source-line-no">460</span><span id="line-460">    if (userProvider.isHadoopSecurityEnabled()) {</span>
<span class="source-line-no">461</span><span id="line-461">      // propagate delegation related props from launcher job to MR job</span>
<span class="source-line-no">462</span><span id="line-462">      if (System.getenv("HADOOP_TOKEN_FILE_LOCATION") != null) {</span>
<span class="source-line-no">463</span><span id="line-463">        job.getConfiguration().set("mapreduce.job.credentials.binary",</span>
<span class="source-line-no">464</span><span id="line-464">          System.getenv("HADOOP_TOKEN_FILE_LOCATION"));</span>
<span class="source-line-no">465</span><span id="line-465">      }</span>
<span class="source-line-no">466</span><span id="line-466">    }</span>
<span class="source-line-no">467</span><span id="line-467"></span>
<span class="source-line-no">468</span><span id="line-468">    if (userProvider.isHBaseSecurityEnabled()) {</span>
<span class="source-line-no">469</span><span id="line-469">      User user = userProvider.getCurrent();</span>
<span class="source-line-no">470</span><span id="line-470">      try {</span>
<span class="source-line-no">471</span><span id="line-471">        // init credentials for remote cluster</span>
<span class="source-line-no">472</span><span id="line-472">        String outputCluster = job.getConfiguration().get(TableOutputFormat.OUTPUT_CLUSTER);</span>
<span class="source-line-no">473</span><span id="line-473">        if (!StringUtils.isBlank(outputCluster)) {</span>
<span class="source-line-no">474</span><span id="line-474">          addTokenForJob(() -&gt; {</span>
<span class="source-line-no">475</span><span id="line-475">            URI uri;</span>
<span class="source-line-no">476</span><span id="line-476">            try {</span>
<span class="source-line-no">477</span><span id="line-477">              uri = new URI(outputCluster);</span>
<span class="source-line-no">478</span><span id="line-478">            } catch (URISyntaxException e) {</span>
<span class="source-line-no">479</span><span id="line-479">              throw new IOException("malformed connection uri: " + outputCluster</span>
<span class="source-line-no">480</span><span id="line-480">                + ", please check config " + TableOutputFormat.OUTPUT_CLUSTER, e);</span>
<span class="source-line-no">481</span><span id="line-481">            }</span>
<span class="source-line-no">482</span><span id="line-482">            return ConnectionFactory.createConnection(uri, job.getConfiguration());</span>
<span class="source-line-no">483</span><span id="line-483">          }, user, job);</span>
<span class="source-line-no">484</span><span id="line-484">        }</span>
<span class="source-line-no">485</span><span id="line-485">        String quorumAddress = job.getConfiguration().get(TableOutputFormat.QUORUM_ADDRESS);</span>
<span class="source-line-no">486</span><span id="line-486">        if (!StringUtils.isBlank(quorumAddress)) {</span>
<span class="source-line-no">487</span><span id="line-487">          addTokenForJob(() -&gt; {</span>
<span class="source-line-no">488</span><span id="line-488">            Configuration peerConf = HBaseConfiguration.createClusterConf(job.getConfiguration(),</span>
<span class="source-line-no">489</span><span id="line-489">              quorumAddress, TableOutputFormat.OUTPUT_CONF_PREFIX);</span>
<span class="source-line-no">490</span><span id="line-490">            return ConnectionFactory.createConnection(peerConf, user);</span>
<span class="source-line-no">491</span><span id="line-491">          }, user, job);</span>
<span class="source-line-no">492</span><span id="line-492">        }</span>
<span class="source-line-no">493</span><span id="line-493">        // init credentials for source cluster</span>
<span class="source-line-no">494</span><span id="line-494">        addTokenForJob(() -&gt; ConnectionFactory.createConnection(job.getConfiguration()), user, job);</span>
<span class="source-line-no">495</span><span id="line-495">      } catch (InterruptedException ie) {</span>
<span class="source-line-no">496</span><span id="line-496">        LOG.info("Interrupted obtaining user authentication token");</span>
<span class="source-line-no">497</span><span id="line-497">        Thread.currentThread().interrupt();</span>
<span class="source-line-no">498</span><span id="line-498">      }</span>
<span class="source-line-no">499</span><span id="line-499">    }</span>
<span class="source-line-no">500</span><span id="line-500">  }</span>
<span class="source-line-no">501</span><span id="line-501"></span>
<span class="source-line-no">502</span><span id="line-502">  /**</span>
<span class="source-line-no">503</span><span id="line-503">   * Obtain an authentication token, for the specified cluster, on behalf of the current user and</span>
<span class="source-line-no">504</span><span id="line-504">   * add it to the credentials for the given map reduce job.</span>
<span class="source-line-no">505</span><span id="line-505">   * @param job  The job that requires the permission.</span>
<span class="source-line-no">506</span><span id="line-506">   * @param conf The configuration to use in connecting to the peer cluster</span>
<span class="source-line-no">507</span><span id="line-507">   * @throws IOException When the authentication token cannot be obtained.</span>
<span class="source-line-no">508</span><span id="line-508">   */</span>
<span class="source-line-no">509</span><span id="line-509">  public static void initCredentialsForCluster(Job job, Configuration conf) throws IOException {</span>
<span class="source-line-no">510</span><span id="line-510">    initCredentialsForCluster(job, conf, null);</span>
<span class="source-line-no">511</span><span id="line-511">  }</span>
<span class="source-line-no">512</span><span id="line-512"></span>
<span class="source-line-no">513</span><span id="line-513">  /**</span>
<span class="source-line-no">514</span><span id="line-514">   * Obtain an authentication token, for the specified cluster, on behalf of the current user and</span>
<span class="source-line-no">515</span><span id="line-515">   * add it to the credentials for the given map reduce job.</span>
<span class="source-line-no">516</span><span id="line-516">   * @param job  The job that requires the permission.</span>
<span class="source-line-no">517</span><span id="line-517">   * @param conf The configuration to use in connecting to the peer cluster</span>
<span class="source-line-no">518</span><span id="line-518">   * @param uri  The connection uri for the given peer cluster</span>
<span class="source-line-no">519</span><span id="line-519">   * @throws IOException When the authentication token cannot be obtained.</span>
<span class="source-line-no">520</span><span id="line-520">   */</span>
<span class="source-line-no">521</span><span id="line-521">  public static void initCredentialsForCluster(Job job, Configuration conf, URI uri)</span>
<span class="source-line-no">522</span><span id="line-522">    throws IOException {</span>
<span class="source-line-no">523</span><span id="line-523">    UserProvider userProvider = UserProvider.instantiate(conf);</span>
<span class="source-line-no">524</span><span id="line-524">    if (userProvider.isHBaseSecurityEnabled()) {</span>
<span class="source-line-no">525</span><span id="line-525">      try {</span>
<span class="source-line-no">526</span><span id="line-526">        addTokenForJob(() -&gt; ConnectionFactory.createConnection(uri, conf),</span>
<span class="source-line-no">527</span><span id="line-527">          userProvider.getCurrent(), job);</span>
<span class="source-line-no">528</span><span id="line-528">      } catch (InterruptedException e) {</span>
<span class="source-line-no">529</span><span id="line-529">        LOG.info("Interrupted obtaining user authentication token");</span>
<span class="source-line-no">530</span><span id="line-530">        Thread.interrupted();</span>
<span class="source-line-no">531</span><span id="line-531">      }</span>
<span class="source-line-no">532</span><span id="line-532">    }</span>
<span class="source-line-no">533</span><span id="line-533">  }</span>
<span class="source-line-no">534</span><span id="line-534"></span>
<span class="source-line-no">535</span><span id="line-535">  /**</span>
<span class="source-line-no">536</span><span id="line-536">   * Writes the given scan into a Base64 encoded string.</span>
<span class="source-line-no">537</span><span id="line-537">   * @param scan The scan to write out.</span>
<span class="source-line-no">538</span><span id="line-538">   * @return The scan saved in a Base64 encoded string.</span>
<span class="source-line-no">539</span><span id="line-539">   * @throws IOException When writing the scan fails.</span>
<span class="source-line-no">540</span><span id="line-540">   */</span>
<span class="source-line-no">541</span><span id="line-541">  public static String convertScanToString(Scan scan) throws IOException {</span>
<span class="source-line-no">542</span><span id="line-542">    ClientProtos.Scan proto = ProtobufUtil.toScan(scan);</span>
<span class="source-line-no">543</span><span id="line-543">    return Bytes.toString(Base64.getEncoder().encode(proto.toByteArray()));</span>
<span class="source-line-no">544</span><span id="line-544">  }</span>
<span class="source-line-no">545</span><span id="line-545"></span>
<span class="source-line-no">546</span><span id="line-546">  /**</span>
<span class="source-line-no">547</span><span id="line-547">   * Converts the given Base64 string back into a Scan instance.</span>
<span class="source-line-no">548</span><span id="line-548">   * @param base64 The scan details.</span>
<span class="source-line-no">549</span><span id="line-549">   * @return The newly created Scan instance.</span>
<span class="source-line-no">550</span><span id="line-550">   * @throws IOException When reading the scan instance fails.</span>
<span class="source-line-no">551</span><span id="line-551">   */</span>
<span class="source-line-no">552</span><span id="line-552">  public static Scan convertStringToScan(String base64) throws IOException {</span>
<span class="source-line-no">553</span><span id="line-553">    byte[] decoded = Base64.getDecoder().decode(base64);</span>
<span class="source-line-no">554</span><span id="line-554">    return ProtobufUtil.toScan(ClientProtos.Scan.parseFrom(decoded));</span>
<span class="source-line-no">555</span><span id="line-555">  }</span>
<span class="source-line-no">556</span><span id="line-556"></span>
<span class="source-line-no">557</span><span id="line-557">  /**</span>
<span class="source-line-no">558</span><span id="line-558">   * Use this before submitting a TableReduce job. It will appropriately set up the JobConf.</span>
<span class="source-line-no">559</span><span id="line-559">   * @param table   The output table.</span>
<span class="source-line-no">560</span><span id="line-560">   * @param reducer The reducer class to use.</span>
<span class="source-line-no">561</span><span id="line-561">   * @param job     The current job to adjust.</span>
<span class="source-line-no">562</span><span id="line-562">   * @throws IOException When determining the region count fails.</span>
<span class="source-line-no">563</span><span id="line-563">   */</span>
<span class="source-line-no">564</span><span id="line-564">  public static void initTableReducerJob(String table, Class&lt;? extends TableReducer&gt; reducer,</span>
<span class="source-line-no">565</span><span id="line-565">    Job job) throws IOException {</span>
<span class="source-line-no">566</span><span id="line-566">    initTableReducerJob(table, reducer, job, null);</span>
<span class="source-line-no">567</span><span id="line-567">  }</span>
<span class="source-line-no">568</span><span id="line-568"></span>
<span class="source-line-no">569</span><span id="line-569">  /**</span>
<span class="source-line-no">570</span><span id="line-570">   * Use this before submitting a TableReduce job. It will appropriately set up the JobConf.</span>
<span class="source-line-no">571</span><span id="line-571">   * @param table       The output table.</span>
<span class="source-line-no">572</span><span id="line-572">   * @param reducer     The reducer class to use.</span>
<span class="source-line-no">573</span><span id="line-573">   * @param job         The current job to adjust.</span>
<span class="source-line-no">574</span><span id="line-574">   * @param partitioner Partitioner to use. Pass &lt;code&gt;null&lt;/code&gt; to use default partitioner.</span>
<span class="source-line-no">575</span><span id="line-575">   * @throws IOException When determining the region count fails.</span>
<span class="source-line-no">576</span><span id="line-576">   */</span>
<span class="source-line-no">577</span><span id="line-577">  public static void initTableReducerJob(String table, Class&lt;? extends TableReducer&gt; reducer,</span>
<span class="source-line-no">578</span><span id="line-578">    Job job, Class partitioner) throws IOException {</span>
<span class="source-line-no">579</span><span id="line-579">    initTableReducerJob(table, reducer, job, partitioner, (URI) null);</span>
<span class="source-line-no">580</span><span id="line-580">  }</span>
<span class="source-line-no">581</span><span id="line-581"></span>
<span class="source-line-no">582</span><span id="line-582">  /**</span>
<span class="source-line-no">583</span><span id="line-583">   * Use this before submitting a TableReduce job. It will appropriately set up the JobConf.</span>
<span class="source-line-no">584</span><span id="line-584">   * @param table         The output table.</span>
<span class="source-line-no">585</span><span id="line-585">   * @param reducer       The reducer class to use.</span>
<span class="source-line-no">586</span><span id="line-586">   * @param job           The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">587</span><span id="line-587">   *                      necessary HBase configuration.</span>
<span class="source-line-no">588</span><span id="line-588">   * @param partitioner   Partitioner to use. Pass &lt;code&gt;null&lt;/code&gt; to use default partitioner.</span>
<span class="source-line-no">589</span><span id="line-589">   * @param quorumAddress Distant cluster to write to; default is null for output to the cluster</span>
<span class="source-line-no">590</span><span id="line-590">   *                      that is designated in &lt;code&gt;hbase-site.xml&lt;/code&gt;. Set this String to the</span>
<span class="source-line-no">591</span><span id="line-591">   *                      zookeeper ensemble of an alternate remote cluster when you would have the</span>
<span class="source-line-no">592</span><span id="line-592">   *                      reduce write a cluster that is other than the default; e.g. copying tables</span>
<span class="source-line-no">593</span><span id="line-593">   *                      between clusters, the source would be designated by</span>
<span class="source-line-no">594</span><span id="line-594">   *                      &lt;code&gt;hbase-site.xml&lt;/code&gt; and this param would have the ensemble address</span>
<span class="source-line-no">595</span><span id="line-595">   *                      of the remote cluster. The format to pass is particular. Pass</span>
<span class="source-line-no">596</span><span id="line-596">   *                      &lt;code&gt; &amp;lt;hbase.zookeeper.quorum&amp;gt;:&amp;lt;</span>
<span class="source-line-no">597</span><span id="line-597">   *             hbase.zookeeper.client.port&amp;gt;:&amp;lt;zookeeper.znode.parent&amp;gt;</span>
<span class="source-line-no">598</span><span id="line-598">   * &lt;/code&gt;           such as &lt;code&gt;server,server2,server3:2181:/hbase&lt;/code&gt;.</span>
<span class="source-line-no">599</span><span id="line-599">   * @throws IOException When determining the region count fails.</span>
<span class="source-line-no">600</span><span id="line-600">   * @deprecated Since 3.0.0, will be removed in 4.0.0. Use</span>
<span class="source-line-no">601</span><span id="line-601">   *             {@link #initTableReducerJob(String, Class, Job, Class, URI)} instead, where we use</span>
<span class="source-line-no">602</span><span id="line-602">   *             the connection uri to specify the target cluster.</span>
<span class="source-line-no">603</span><span id="line-603">   */</span>
<span class="source-line-no">604</span><span id="line-604">  @Deprecated</span>
<span class="source-line-no">605</span><span id="line-605">  public static void initTableReducerJob(String table, Class&lt;? extends TableReducer&gt; reducer,</span>
<span class="source-line-no">606</span><span id="line-606">    Job job, Class partitioner, String quorumAddress) throws IOException {</span>
<span class="source-line-no">607</span><span id="line-607">    initTableReducerJob(table, reducer, job, partitioner, quorumAddress, true);</span>
<span class="source-line-no">608</span><span id="line-608">  }</span>
<span class="source-line-no">609</span><span id="line-609"></span>
<span class="source-line-no">610</span><span id="line-610">  /**</span>
<span class="source-line-no">611</span><span id="line-611">   * Use this before submitting a TableReduce job. It will appropriately set up the JobConf.</span>
<span class="source-line-no">612</span><span id="line-612">   * @param table             The output table.</span>
<span class="source-line-no">613</span><span id="line-613">   * @param reducer           The reducer class to use.</span>
<span class="source-line-no">614</span><span id="line-614">   * @param job               The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">615</span><span id="line-615">   *                          necessary HBase configuration.</span>
<span class="source-line-no">616</span><span id="line-616">   * @param partitioner       Partitioner to use. Pass &lt;code&gt;null&lt;/code&gt; to use default partitioner.</span>
<span class="source-line-no">617</span><span id="line-617">   * @param quorumAddress     Distant cluster to write to; default is null for output to the cluster</span>
<span class="source-line-no">618</span><span id="line-618">   *                          that is designated in &lt;code&gt;hbase-site.xml&lt;/code&gt;. Set this String to</span>
<span class="source-line-no">619</span><span id="line-619">   *                          the zookeeper ensemble of an alternate remote cluster when you would</span>
<span class="source-line-no">620</span><span id="line-620">   *                          have the reduce write a cluster that is other than the default; e.g.</span>
<span class="source-line-no">621</span><span id="line-621">   *                          copying tables between clusters, the source would be designated by</span>
<span class="source-line-no">622</span><span id="line-622">   *                          &lt;code&gt;hbase-site.xml&lt;/code&gt; and this param would have the ensemble</span>
<span class="source-line-no">623</span><span id="line-623">   *                          address of the remote cluster. The format to pass is particular. Pass</span>
<span class="source-line-no">624</span><span id="line-624">   *                          &lt;code&gt; &amp;lt;hbase.zookeeper.quorum&amp;gt;:&amp;lt;</span>
<span class="source-line-no">625</span><span id="line-625">   *             hbase.zookeeper.client.port&amp;gt;:&amp;lt;zookeeper.znode.parent&amp;gt;</span>
<span class="source-line-no">626</span><span id="line-626">   * &lt;/code&gt;               such as &lt;code&gt;server,server2,server3:2181:/hbase&lt;/code&gt;.</span>
<span class="source-line-no">627</span><span id="line-627">   * @param addDependencyJars upload HBase jars and jars for any of the configured job classes via</span>
<span class="source-line-no">628</span><span id="line-628">   *                          the distributed cache (tmpjars).</span>
<span class="source-line-no">629</span><span id="line-629">   * @throws IOException When determining the region count fails.</span>
<span class="source-line-no">630</span><span id="line-630">   * @deprecated Since 3.0.0, will be removed in 4.0.0. Use</span>
<span class="source-line-no">631</span><span id="line-631">   *             {@link #initTableReducerJob(String, Class, Job, Class, URI, boolean)} instead,</span>
<span class="source-line-no">632</span><span id="line-632">   *             where we use the connection uri to specify the target cluster.</span>
<span class="source-line-no">633</span><span id="line-633">   */</span>
<span class="source-line-no">634</span><span id="line-634">  @Deprecated</span>
<span class="source-line-no">635</span><span id="line-635">  public static void initTableReducerJob(String table, Class&lt;? extends TableReducer&gt; reducer,</span>
<span class="source-line-no">636</span><span id="line-636">    Job job, Class partitioner, String quorumAddress, boolean addDependencyJars)</span>
<span class="source-line-no">637</span><span id="line-637">    throws IOException {</span>
<span class="source-line-no">638</span><span id="line-638">    initTableReducerJob(table, reducer, job, partitioner, () -&gt; {</span>
<span class="source-line-no">639</span><span id="line-639">      // If passed a quorum/ensemble address, pass it on to TableOutputFormat.</span>
<span class="source-line-no">640</span><span id="line-640">      if (quorumAddress != null) {</span>
<span class="source-line-no">641</span><span id="line-641">        // Calling this will validate the format</span>
<span class="source-line-no">642</span><span id="line-642">        ZKConfig.validateClusterKey(quorumAddress);</span>
<span class="source-line-no">643</span><span id="line-643">        job.getConfiguration().set(TableOutputFormat.QUORUM_ADDRESS, quorumAddress);</span>
<span class="source-line-no">644</span><span id="line-644">      }</span>
<span class="source-line-no">645</span><span id="line-645">    }, addDependencyJars);</span>
<span class="source-line-no">646</span><span id="line-646">  }</span>
<span class="source-line-no">647</span><span id="line-647"></span>
<span class="source-line-no">648</span><span id="line-648">  /**</span>
<span class="source-line-no">649</span><span id="line-649">   * Use this before submitting a TableReduce job. It will appropriately set up the JobConf.</span>
<span class="source-line-no">650</span><span id="line-650">   * @param table         The output table.</span>
<span class="source-line-no">651</span><span id="line-651">   * @param reducer       The reducer class to use.</span>
<span class="source-line-no">652</span><span id="line-652">   * @param job           The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">653</span><span id="line-653">   *                      necessary HBase configuration.</span>
<span class="source-line-no">654</span><span id="line-654">   * @param partitioner   Partitioner to use. Pass &lt;code&gt;null&lt;/code&gt; to use default partitioner.</span>
<span class="source-line-no">655</span><span id="line-655">   * @param outputCluster The HBase cluster you want to write to. Default is null which means output</span>
<span class="source-line-no">656</span><span id="line-656">   *                      to the same cluster you read from, i.e, the cluster when initializing by</span>
<span class="source-line-no">657</span><span id="line-657">   *                      the job's Configuration instance.</span>
<span class="source-line-no">658</span><span id="line-658">   * @throws IOException When determining the region count fails.</span>
<span class="source-line-no">659</span><span id="line-659">   */</span>
<span class="source-line-no">660</span><span id="line-660">  public static void initTableReducerJob(String table, Class&lt;? extends TableReducer&gt; reducer,</span>
<span class="source-line-no">661</span><span id="line-661">    Job job, Class partitioner, URI outputCluster) throws IOException {</span>
<span class="source-line-no">662</span><span id="line-662">    initTableReducerJob(table, reducer, job, partitioner, outputCluster, true);</span>
<span class="source-line-no">663</span><span id="line-663">  }</span>
<span class="source-line-no">664</span><span id="line-664"></span>
<span class="source-line-no">665</span><span id="line-665">  /**</span>
<span class="source-line-no">666</span><span id="line-666">   * Use this before submitting a TableReduce job. It will appropriately set up the JobConf.</span>
<span class="source-line-no">667</span><span id="line-667">   * @param table             The output table.</span>
<span class="source-line-no">668</span><span id="line-668">   * @param reducer           The reducer class to use.</span>
<span class="source-line-no">669</span><span id="line-669">   * @param job               The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">670</span><span id="line-670">   *                          necessary HBase configuration.</span>
<span class="source-line-no">671</span><span id="line-671">   * @param partitioner       Partitioner to use. Pass &lt;code&gt;null&lt;/code&gt; to use default partitioner.</span>
<span class="source-line-no">672</span><span id="line-672">   * @param outputCluster     The HBase cluster you want to write to. Default is null which means</span>
<span class="source-line-no">673</span><span id="line-673">   *                          output to the same cluster you read from, i.e, the cluster when</span>
<span class="source-line-no">674</span><span id="line-674">   *                          initializing by the job's Configuration instance.</span>
<span class="source-line-no">675</span><span id="line-675">   * @param addDependencyJars upload HBase jars and jars for any of the configured job classes via</span>
<span class="source-line-no">676</span><span id="line-676">   *                          the distributed cache (tmpjars).</span>
<span class="source-line-no">677</span><span id="line-677">   * @throws IOException When determining the region count fails.</span>
<span class="source-line-no">678</span><span id="line-678">   */</span>
<span class="source-line-no">679</span><span id="line-679">  public static void initTableReducerJob(String table, Class&lt;? extends TableReducer&gt; reducer,</span>
<span class="source-line-no">680</span><span id="line-680">    Job job, Class partitioner, URI outputCluster, boolean addDependencyJars) throws IOException {</span>
<span class="source-line-no">681</span><span id="line-681">    initTableReducerJob(table, reducer, job, partitioner, () -&gt; {</span>
<span class="source-line-no">682</span><span id="line-682">      if (outputCluster != null) {</span>
<span class="source-line-no">683</span><span id="line-683">        ConnectionRegistryFactory.validate(outputCluster);</span>
<span class="source-line-no">684</span><span id="line-684">        job.getConfiguration().set(TableOutputFormat.OUTPUT_CLUSTER, outputCluster.toString());</span>
<span class="source-line-no">685</span><span id="line-685">      }</span>
<span class="source-line-no">686</span><span id="line-686">    }, addDependencyJars);</span>
<span class="source-line-no">687</span><span id="line-687">  }</span>
<span class="source-line-no">688</span><span id="line-688"></span>
<span class="source-line-no">689</span><span id="line-689">  private static void initTableReducerJob(String table, Class&lt;? extends TableReducer&gt; reducer,</span>
<span class="source-line-no">690</span><span id="line-690">    Job job, Class partitioner, IOExceptionRunnable setOutputCluster, boolean addDependencyJars)</span>
<span class="source-line-no">691</span><span id="line-691">    throws IOException {</span>
<span class="source-line-no">692</span><span id="line-692">    Configuration conf = job.getConfiguration();</span>
<span class="source-line-no">693</span><span id="line-693">    HBaseConfiguration.merge(conf, HBaseConfiguration.create(conf));</span>
<span class="source-line-no">694</span><span id="line-694">    job.setOutputFormatClass(TableOutputFormat.class);</span>
<span class="source-line-no">695</span><span id="line-695">    if (reducer != null) {</span>
<span class="source-line-no">696</span><span id="line-696">      job.setReducerClass(reducer);</span>
<span class="source-line-no">697</span><span id="line-697">    }</span>
<span class="source-line-no">698</span><span id="line-698">    conf.set(TableOutputFormat.OUTPUT_TABLE, table);</span>
<span class="source-line-no">699</span><span id="line-699">    conf.setStrings("io.serializations", conf.get("io.serializations"),</span>
<span class="source-line-no">700</span><span id="line-700">      MutationSerialization.class.getName(), ResultSerialization.class.getName());</span>
<span class="source-line-no">701</span><span id="line-701">    setOutputCluster.run();</span>
<span class="source-line-no">702</span><span id="line-702">    job.setOutputKeyClass(ImmutableBytesWritable.class);</span>
<span class="source-line-no">703</span><span id="line-703">    job.setOutputValueClass(Writable.class);</span>
<span class="source-line-no">704</span><span id="line-704">    if (partitioner == HRegionPartitioner.class) {</span>
<span class="source-line-no">705</span><span id="line-705">      job.setPartitionerClass(HRegionPartitioner.class);</span>
<span class="source-line-no">706</span><span id="line-706">      int regions = getRegionCount(conf, TableName.valueOf(table));</span>
<span class="source-line-no">707</span><span id="line-707">      if (job.getNumReduceTasks() &gt; regions) {</span>
<span class="source-line-no">708</span><span id="line-708">        job.setNumReduceTasks(regions);</span>
<span class="source-line-no">709</span><span id="line-709">      }</span>
<span class="source-line-no">710</span><span id="line-710">    } else if (partitioner != null) {</span>
<span class="source-line-no">711</span><span id="line-711">      job.setPartitionerClass(partitioner);</span>
<span class="source-line-no">712</span><span id="line-712">    }</span>
<span class="source-line-no">713</span><span id="line-713"></span>
<span class="source-line-no">714</span><span id="line-714">    if (addDependencyJars) {</span>
<span class="source-line-no">715</span><span id="line-715">      addDependencyJars(job);</span>
<span class="source-line-no">716</span><span id="line-716">    }</span>
<span class="source-line-no">717</span><span id="line-717"></span>
<span class="source-line-no">718</span><span id="line-718">    initCredentials(job);</span>
<span class="source-line-no">719</span><span id="line-719">  }</span>
<span class="source-line-no">720</span><span id="line-720"></span>
<span class="source-line-no">721</span><span id="line-721">  /**</span>
<span class="source-line-no">722</span><span id="line-722">   * Use this before submitting a TableReduce job. It will appropriately set up the JobConf.</span>
<span class="source-line-no">723</span><span id="line-723">   * @param table         The output table.</span>
<span class="source-line-no">724</span><span id="line-724">   * @param reducer       The reducer class to use.</span>
<span class="source-line-no">725</span><span id="line-725">   * @param job           The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">726</span><span id="line-726">   *                      necessary HBase configuration.</span>
<span class="source-line-no">727</span><span id="line-727">   * @param partitioner   Partitioner to use. Pass &lt;code&gt;null&lt;/code&gt; to use default partitioner.</span>
<span class="source-line-no">728</span><span id="line-728">   * @param quorumAddress Distant cluster to write to; default is null for output to the cluster</span>
<span class="source-line-no">729</span><span id="line-729">   *                      that is designated in &lt;code&gt;hbase-site.xml&lt;/code&gt;. Set this String to the</span>
<span class="source-line-no">730</span><span id="line-730">   *                      zookeeper ensemble of an alternate remote cluster when you would have the</span>
<span class="source-line-no">731</span><span id="line-731">   *                      reduce write a cluster that is other than the default; e.g. copying tables</span>
<span class="source-line-no">732</span><span id="line-732">   *                      between clusters, the source would be designated by</span>
<span class="source-line-no">733</span><span id="line-733">   *                      &lt;code&gt;hbase-site.xml&lt;/code&gt; and this param would have the ensemble address</span>
<span class="source-line-no">734</span><span id="line-734">   *                      of the remote cluster. The format to pass is particular. Pass</span>
<span class="source-line-no">735</span><span id="line-735">   *                      &lt;code&gt; &amp;lt;hbase.zookeeper.quorum&amp;gt;:&amp;lt;</span>
<span class="source-line-no">736</span><span id="line-736">   *             hbase.zookeeper.client.port&amp;gt;:&amp;lt;zookeeper.znode.parent&amp;gt;</span>
<span class="source-line-no">737</span><span id="line-737">   * &lt;/code&gt;           such as &lt;code&gt;server,server2,server3:2181:/hbase&lt;/code&gt;.</span>
<span class="source-line-no">738</span><span id="line-738">   * @param serverClass   redefined hbase.regionserver.class</span>
<span class="source-line-no">739</span><span id="line-739">   * @param serverImpl    redefined hbase.regionserver.impl</span>
<span class="source-line-no">740</span><span id="line-740">   * @throws IOException When determining the region count fails.</span>
<span class="source-line-no">741</span><span id="line-741">   * @deprecated Since 2.5.9, 2.6.1, 2.7.0, will be removed in 4.0.0. The {@code serverClass} and</span>
<span class="source-line-no">742</span><span id="line-742">   *             {@code serverImpl} do not take effect any more, just use</span>
<span class="source-line-no">743</span><span id="line-743">   *             {@link #initTableReducerJob(String, Class, Job, Class, String)} instead.</span>
<span class="source-line-no">744</span><span id="line-744">   * @see #initTableReducerJob(String, Class, Job, Class, String)</span>
<span class="source-line-no">745</span><span id="line-745">   */</span>
<span class="source-line-no">746</span><span id="line-746">  @Deprecated</span>
<span class="source-line-no">747</span><span id="line-747">  public static void initTableReducerJob(String table, Class&lt;? extends TableReducer&gt; reducer,</span>
<span class="source-line-no">748</span><span id="line-748">    Job job, Class partitioner, String quorumAddress, String serverClass, String serverImpl)</span>
<span class="source-line-no">749</span><span id="line-749">    throws IOException {</span>
<span class="source-line-no">750</span><span id="line-750">    initTableReducerJob(table, reducer, job, partitioner, quorumAddress);</span>
<span class="source-line-no">751</span><span id="line-751">  }</span>
<span class="source-line-no">752</span><span id="line-752"></span>
<span class="source-line-no">753</span><span id="line-753">  /**</span>
<span class="source-line-no">754</span><span id="line-754">   * Use this before submitting a TableReduce job. It will appropriately set up the JobConf.</span>
<span class="source-line-no">755</span><span id="line-755">   * @param table             The output table.</span>
<span class="source-line-no">756</span><span id="line-756">   * @param reducer           The reducer class to use.</span>
<span class="source-line-no">757</span><span id="line-757">   * @param job               The current job to adjust. Make sure the passed job is carrying all</span>
<span class="source-line-no">758</span><span id="line-758">   *                          necessary HBase configuration.</span>
<span class="source-line-no">759</span><span id="line-759">   * @param partitioner       Partitioner to use. Pass &lt;code&gt;null&lt;/code&gt; to use default partitioner.</span>
<span class="source-line-no">760</span><span id="line-760">   * @param quorumAddress     Distant cluster to write to; default is null for output to the cluster</span>
<span class="source-line-no">761</span><span id="line-761">   *                          that is designated in &lt;code&gt;hbase-site.xml&lt;/code&gt;. Set this String to</span>
<span class="source-line-no">762</span><span id="line-762">   *                          the zookeeper ensemble of an alternate remote cluster when you would</span>
<span class="source-line-no">763</span><span id="line-763">   *                          have the reduce write a cluster that is other than the default; e.g.</span>
<span class="source-line-no">764</span><span id="line-764">   *                          copying tables between clusters, the source would be designated by</span>
<span class="source-line-no">765</span><span id="line-765">   *                          &lt;code&gt;hbase-site.xml&lt;/code&gt; and this param would have the ensemble</span>
<span class="source-line-no">766</span><span id="line-766">   *                          address of the remote cluster. The format to pass is particular. Pass</span>
<span class="source-line-no">767</span><span id="line-767">   *                          &lt;code&gt; &amp;lt;hbase.zookeeper.quorum&amp;gt;:&amp;lt;</span>
<span class="source-line-no">768</span><span id="line-768">   *             hbase.zookeeper.client.port&amp;gt;:&amp;lt;zookeeper.znode.parent&amp;gt;</span>
<span class="source-line-no">769</span><span id="line-769">   * &lt;/code&gt;               such as &lt;code&gt;server,server2,server3:2181:/hbase&lt;/code&gt;.</span>
<span class="source-line-no">770</span><span id="line-770">   * @param serverClass       redefined hbase.regionserver.class</span>
<span class="source-line-no">771</span><span id="line-771">   * @param serverImpl        redefined hbase.regionserver.impl</span>
<span class="source-line-no">772</span><span id="line-772">   * @param addDependencyJars upload HBase jars and jars for any of the configured job classes via</span>
<span class="source-line-no">773</span><span id="line-773">   *                          the distributed cache (tmpjars).</span>
<span class="source-line-no">774</span><span id="line-774">   * @throws IOException When determining the region count fails.</span>
<span class="source-line-no">775</span><span id="line-775">   * @deprecated Since 2.5.9, 2.6.1, 2.7.0, will be removed in 4.0.0. The {@code serverClass} and</span>
<span class="source-line-no">776</span><span id="line-776">   *             {@code serverImpl} do not take effect any more, just use</span>
<span class="source-line-no">777</span><span id="line-777">   *             {@link #initTableReducerJob(String, Class, Job, Class, String, boolean)} instead.</span>
<span class="source-line-no">778</span><span id="line-778">   * @see #initTableReducerJob(String, Class, Job, Class, String, boolean)</span>
<span class="source-line-no">779</span><span id="line-779">   */</span>
<span class="source-line-no">780</span><span id="line-780">  @Deprecated</span>
<span class="source-line-no">781</span><span id="line-781">  public static void initTableReducerJob(String table, Class&lt;? extends TableReducer&gt; reducer,</span>
<span class="source-line-no">782</span><span id="line-782">    Job job, Class partitioner, String quorumAddress, String serverClass, String serverImpl,</span>
<span class="source-line-no">783</span><span id="line-783">    boolean addDependencyJars) throws IOException {</span>
<span class="source-line-no">784</span><span id="line-784">    initTableReducerJob(table, reducer, job, partitioner, quorumAddress, addDependencyJars);</span>
<span class="source-line-no">785</span><span id="line-785">  }</span>
<span class="source-line-no">786</span><span id="line-786"></span>
<span class="source-line-no">787</span><span id="line-787">  /**</span>
<span class="source-line-no">788</span><span id="line-788">   * Ensures that the given number of reduce tasks for the given job configuration does not exceed</span>
<span class="source-line-no">789</span><span id="line-789">   * the number of regions for the given table.</span>
<span class="source-line-no">790</span><span id="line-790">   * @param table The table to get the region count for.</span>
<span class="source-line-no">791</span><span id="line-791">   * @param job   The current job to adjust.</span>
<span class="source-line-no">792</span><span id="line-792">   * @throws IOException When retrieving the table details fails.</span>
<span class="source-line-no">793</span><span id="line-793">   */</span>
<span class="source-line-no">794</span><span id="line-794">  public static void limitNumReduceTasks(String table, Job job) throws IOException {</span>
<span class="source-line-no">795</span><span id="line-795">    int regions = getRegionCount(job.getConfiguration(), TableName.valueOf(table));</span>
<span class="source-line-no">796</span><span id="line-796">    if (job.getNumReduceTasks() &gt; regions) {</span>
<span class="source-line-no">797</span><span id="line-797">      job.setNumReduceTasks(regions);</span>
<span class="source-line-no">798</span><span id="line-798">    }</span>
<span class="source-line-no">799</span><span id="line-799">  }</span>
<span class="source-line-no">800</span><span id="line-800"></span>
<span class="source-line-no">801</span><span id="line-801">  /**</span>
<span class="source-line-no">802</span><span id="line-802">   * Sets the number of reduce tasks for the given job configuration to the number of regions the</span>
<span class="source-line-no">803</span><span id="line-803">   * given table has.</span>
<span class="source-line-no">804</span><span id="line-804">   * @param table The table to get the region count for.</span>
<span class="source-line-no">805</span><span id="line-805">   * @param job   The current job to adjust.</span>
<span class="source-line-no">806</span><span id="line-806">   * @throws IOException When retrieving the table details fails.</span>
<span class="source-line-no">807</span><span id="line-807">   */</span>
<span class="source-line-no">808</span><span id="line-808">  public static void setNumReduceTasks(String table, Job job) throws IOException {</span>
<span class="source-line-no">809</span><span id="line-809">    job.setNumReduceTasks(getRegionCount(job.getConfiguration(), TableName.valueOf(table)));</span>
<span class="source-line-no">810</span><span id="line-810">  }</span>
<span class="source-line-no">811</span><span id="line-811"></span>
<span class="source-line-no">812</span><span id="line-812">  /**</span>
<span class="source-line-no">813</span><span id="line-813">   * Sets the number of rows to return and cache with each scanner iteration. Higher caching values</span>
<span class="source-line-no">814</span><span id="line-814">   * will enable faster mapreduce jobs at the expense of requiring more heap to contain the cached</span>
<span class="source-line-no">815</span><span id="line-815">   * rows.</span>
<span class="source-line-no">816</span><span id="line-816">   * @param job       The current job to adjust.</span>
<span class="source-line-no">817</span><span id="line-817">   * @param batchSize The number of rows to return in batch with each scanner iteration.</span>
<span class="source-line-no">818</span><span id="line-818">   */</span>
<span class="source-line-no">819</span><span id="line-819">  public static void setScannerCaching(Job job, int batchSize) {</span>
<span class="source-line-no">820</span><span id="line-820">    job.getConfiguration().setInt("hbase.client.scanner.caching", batchSize);</span>
<span class="source-line-no">821</span><span id="line-821">  }</span>
<span class="source-line-no">822</span><span id="line-822"></span>
<span class="source-line-no">823</span><span id="line-823">  /**</span>
<span class="source-line-no">824</span><span id="line-824">   * Add HBase and its dependencies (only) to the job configuration.</span>
<span class="source-line-no">825</span><span id="line-825">   * &lt;p&gt;</span>
<span class="source-line-no">826</span><span id="line-826">   * This is intended as a low-level API, facilitating code reuse between this class and its mapred</span>
<span class="source-line-no">827</span><span id="line-827">   * counterpart. It also of use to external tools that need to build a MapReduce job that interacts</span>
<span class="source-line-no">828</span><span id="line-828">   * with HBase but want fine-grained control over the jars shipped to the cluster.</span>
<span class="source-line-no">829</span><span id="line-829">   * &lt;/p&gt;</span>
<span class="source-line-no">830</span><span id="line-830">   * @param conf The Configuration object to extend with dependencies.</span>
<span class="source-line-no">831</span><span id="line-831">   * @see org.apache.hadoop.hbase.mapred.TableMapReduceUtil</span>
<span class="source-line-no">832</span><span id="line-832">   * @see &lt;a href="https://issues.apache.org/jira/browse/PIG-3285"&gt;PIG-3285&lt;/a&gt;</span>
<span class="source-line-no">833</span><span id="line-833">   */</span>
<span class="source-line-no">834</span><span id="line-834">  public static void addHBaseDependencyJars(Configuration conf) throws IOException {</span>
<span class="source-line-no">835</span><span id="line-835">    addDependencyJarsForClasses(conf,</span>
<span class="source-line-no">836</span><span id="line-836">      // explicitly pull a class from each module</span>
<span class="source-line-no">837</span><span id="line-837">      org.apache.hadoop.hbase.HConstants.class, // hbase-common</span>
<span class="source-line-no">838</span><span id="line-838">      org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.class, // hbase-protocol-shaded</span>
<span class="source-line-no">839</span><span id="line-839">      org.apache.hadoop.hbase.client.Put.class, // hbase-client</span>
<span class="source-line-no">840</span><span id="line-840">      org.apache.hadoop.hbase.ipc.RpcServer.class, // hbase-server</span>
<span class="source-line-no">841</span><span id="line-841">      org.apache.hadoop.hbase.CompatibilityFactory.class, // hbase-hadoop-compat</span>
<span class="source-line-no">842</span><span id="line-842">      org.apache.hadoop.hbase.mapreduce.JobUtil.class, // hbase-hadoop2-compat</span>
<span class="source-line-no">843</span><span id="line-843">      org.apache.hadoop.hbase.mapreduce.TableMapper.class, // hbase-mapreduce</span>
<span class="source-line-no">844</span><span id="line-844">      org.apache.hadoop.hbase.metrics.impl.FastLongHistogram.class, // hbase-metrics</span>
<span class="source-line-no">845</span><span id="line-845">      org.apache.hadoop.hbase.metrics.Snapshot.class, // hbase-metrics-api</span>
<span class="source-line-no">846</span><span id="line-846">      org.apache.hadoop.hbase.replication.ReplicationUtils.class, // hbase-replication</span>
<span class="source-line-no">847</span><span id="line-847">      org.apache.hadoop.hbase.http.HttpServer.class, // hbase-http</span>
<span class="source-line-no">848</span><span id="line-848">      org.apache.hadoop.hbase.procedure2.Procedure.class, // hbase-procedure</span>
<span class="source-line-no">849</span><span id="line-849">      org.apache.hadoop.hbase.zookeeper.ZKWatcher.class, // hbase-zookeeper</span>
<span class="source-line-no">850</span><span id="line-850">      org.apache.hbase.thirdparty.com.google.common.collect.Lists.class, // hb-shaded-miscellaneous</span>
<span class="source-line-no">851</span><span id="line-851">      org.apache.hbase.thirdparty.com.google.gson.GsonBuilder.class, // hbase-shaded-gson</span>
<span class="source-line-no">852</span><span id="line-852">      org.apache.hbase.thirdparty.com.google.protobuf.UnsafeByteOperations.class, // hb-sh-protobuf</span>
<span class="source-line-no">853</span><span id="line-853">      org.apache.hbase.thirdparty.io.netty.channel.Channel.class, // hbase-shaded-netty</span>
<span class="source-line-no">854</span><span id="line-854">      org.apache.hadoop.hbase.unsafe.HBasePlatformDependent.class, // hbase-unsafe</span>
<span class="source-line-no">855</span><span id="line-855">      org.apache.zookeeper.ZooKeeper.class, // zookeeper</span>
<span class="source-line-no">856</span><span id="line-856">      com.codahale.metrics.MetricRegistry.class, // metrics-core</span>
<span class="source-line-no">857</span><span id="line-857">      org.apache.commons.lang3.ArrayUtils.class, // commons-lang</span>
<span class="source-line-no">858</span><span id="line-858">      io.opentelemetry.api.trace.Span.class, // opentelemetry-api</span>
<span class="source-line-no">859</span><span id="line-859">      io.opentelemetry.semconv.SemanticAttributes.class, // opentelemetry-semconv</span>
<span class="source-line-no">860</span><span id="line-860">      io.opentelemetry.context.Context.class); // opentelemetry-context</span>
<span class="source-line-no">861</span><span id="line-861">  }</span>
<span class="source-line-no">862</span><span id="line-862"></span>
<span class="source-line-no">863</span><span id="line-863">  /**</span>
<span class="source-line-no">864</span><span id="line-864">   * Returns a classpath string built from the content of the "tmpjars" value in {@code conf}. Also</span>
<span class="source-line-no">865</span><span id="line-865">   * exposed to shell scripts via `bin/hbase mapredcp`.</span>
<span class="source-line-no">866</span><span id="line-866">   */</span>
<span class="source-line-no">867</span><span id="line-867">  public static String buildDependencyClasspath(Configuration conf) {</span>
<span class="source-line-no">868</span><span id="line-868">    if (conf == null) {</span>
<span class="source-line-no">869</span><span id="line-869">      throw new IllegalArgumentException("Must provide a configuration object.");</span>
<span class="source-line-no">870</span><span id="line-870">    }</span>
<span class="source-line-no">871</span><span id="line-871">    Set&lt;String&gt; paths = new HashSet&lt;&gt;(conf.getStringCollection("tmpjars"));</span>
<span class="source-line-no">872</span><span id="line-872">    if (paths.isEmpty()) {</span>
<span class="source-line-no">873</span><span id="line-873">      throw new IllegalArgumentException("Configuration contains no tmpjars.");</span>
<span class="source-line-no">874</span><span id="line-874">    }</span>
<span class="source-line-no">875</span><span id="line-875">    StringBuilder sb = new StringBuilder();</span>
<span class="source-line-no">876</span><span id="line-876">    for (String s : paths) {</span>
<span class="source-line-no">877</span><span id="line-877">      // entries can take the form 'file:/path/to/file.jar'.</span>
<span class="source-line-no">878</span><span id="line-878">      int idx = s.indexOf(":");</span>
<span class="source-line-no">879</span><span id="line-879">      if (idx != -1) s = s.substring(idx + 1);</span>
<span class="source-line-no">880</span><span id="line-880">      if (sb.length() &gt; 0) sb.append(File.pathSeparator);</span>
<span class="source-line-no">881</span><span id="line-881">      sb.append(s);</span>
<span class="source-line-no">882</span><span id="line-882">    }</span>
<span class="source-line-no">883</span><span id="line-883">    return sb.toString();</span>
<span class="source-line-no">884</span><span id="line-884">  }</span>
<span class="source-line-no">885</span><span id="line-885"></span>
<span class="source-line-no">886</span><span id="line-886">  /**</span>
<span class="source-line-no">887</span><span id="line-887">   * Add the HBase dependency jars as well as jars for any of the configured job classes to the job</span>
<span class="source-line-no">888</span><span id="line-888">   * configuration, so that JobClient will ship them to the cluster and add them to the</span>
<span class="source-line-no">889</span><span id="line-889">   * DistributedCache.</span>
<span class="source-line-no">890</span><span id="line-890">   */</span>
<span class="source-line-no">891</span><span id="line-891">  public static void addDependencyJars(Job job) throws IOException {</span>
<span class="source-line-no">892</span><span id="line-892">    addHBaseDependencyJars(job.getConfiguration());</span>
<span class="source-line-no">893</span><span id="line-893">    try {</span>
<span class="source-line-no">894</span><span id="line-894">      addDependencyJarsForClasses(job.getConfiguration(),</span>
<span class="source-line-no">895</span><span id="line-895">        // when making changes here, consider also mapred.TableMapReduceUtil</span>
<span class="source-line-no">896</span><span id="line-896">        // pull job classes</span>
<span class="source-line-no">897</span><span id="line-897">        job.getMapOutputKeyClass(), job.getMapOutputValueClass(), job.getInputFormatClass(),</span>
<span class="source-line-no">898</span><span id="line-898">        job.getOutputKeyClass(), job.getOutputValueClass(), job.getOutputFormatClass(),</span>
<span class="source-line-no">899</span><span id="line-899">        job.getPartitionerClass(), job.getCombinerClass());</span>
<span class="source-line-no">900</span><span id="line-900">    } catch (ClassNotFoundException e) {</span>
<span class="source-line-no">901</span><span id="line-901">      throw new IOException(e);</span>
<span class="source-line-no">902</span><span id="line-902">    }</span>
<span class="source-line-no">903</span><span id="line-903">  }</span>
<span class="source-line-no">904</span><span id="line-904"></span>
<span class="source-line-no">905</span><span id="line-905">  /**</span>
<span class="source-line-no">906</span><span id="line-906">   * Add the jars containing the given classes to the job's configuration such that JobClient will</span>
<span class="source-line-no">907</span><span id="line-907">   * ship them to the cluster and add them to the DistributedCache. N.B. that this method at most</span>
<span class="source-line-no">908</span><span id="line-908">   * adds one jar per class given. If there is more than one jar available containing a class with</span>
<span class="source-line-no">909</span><span id="line-909">   * the same name as a given class, we don't define which of those jars might be chosen.</span>
<span class="source-line-no">910</span><span id="line-910">   * @param conf    The Hadoop Configuration to modify</span>
<span class="source-line-no">911</span><span id="line-911">   * @param classes will add just those dependencies needed to find the given classes</span>
<span class="source-line-no">912</span><span id="line-912">   * @throws IOException if an underlying library call fails.</span>
<span class="source-line-no">913</span><span id="line-913">   */</span>
<span class="source-line-no">914</span><span id="line-914">  @InterfaceAudience.Private</span>
<span class="source-line-no">915</span><span id="line-915">  public static void addDependencyJarsForClasses(Configuration conf, Class&lt;?&gt;... classes)</span>
<span class="source-line-no">916</span><span id="line-916">    throws IOException {</span>
<span class="source-line-no">917</span><span id="line-917"></span>
<span class="source-line-no">918</span><span id="line-918">    FileSystem localFs = FileSystem.getLocal(conf);</span>
<span class="source-line-no">919</span><span id="line-919">    Set&lt;String&gt; jars = new HashSet&lt;&gt;();</span>
<span class="source-line-no">920</span><span id="line-920">    // Add jars that are already in the tmpjars variable</span>
<span class="source-line-no">921</span><span id="line-921">    jars.addAll(conf.getStringCollection("tmpjars"));</span>
<span class="source-line-no">922</span><span id="line-922"></span>
<span class="source-line-no">923</span><span id="line-923">    // add jars as we find them to a map of contents jar name so that we can avoid</span>
<span class="source-line-no">924</span><span id="line-924">    // creating new jars for classes that have already been packaged.</span>
<span class="source-line-no">925</span><span id="line-925">    Map&lt;String, String&gt; packagedClasses = new HashMap&lt;&gt;();</span>
<span class="source-line-no">926</span><span id="line-926"></span>
<span class="source-line-no">927</span><span id="line-927">    // Add jars containing the specified classes</span>
<span class="source-line-no">928</span><span id="line-928">    for (Class&lt;?&gt; clazz : classes) {</span>
<span class="source-line-no">929</span><span id="line-929">      if (clazz == null) continue;</span>
<span class="source-line-no">930</span><span id="line-930"></span>
<span class="source-line-no">931</span><span id="line-931">      Path path = findOrCreateJar(clazz, localFs, packagedClasses);</span>
<span class="source-line-no">932</span><span id="line-932">      if (path == null) {</span>
<span class="source-line-no">933</span><span id="line-933">        LOG.warn("Could not find jar for class " + clazz + " in order to ship it to the cluster.");</span>
<span class="source-line-no">934</span><span id="line-934">        continue;</span>
<span class="source-line-no">935</span><span id="line-935">      }</span>
<span class="source-line-no">936</span><span id="line-936">      if (!localFs.exists(path)) {</span>
<span class="source-line-no">937</span><span id="line-937">        LOG.warn("Could not validate jar file " + path + " for class " + clazz);</span>
<span class="source-line-no">938</span><span id="line-938">        continue;</span>
<span class="source-line-no">939</span><span id="line-939">      }</span>
<span class="source-line-no">940</span><span id="line-940">      jars.add(path.toString());</span>
<span class="source-line-no">941</span><span id="line-941">    }</span>
<span class="source-line-no">942</span><span id="line-942">    if (jars.isEmpty()) {</span>
<span class="source-line-no">943</span><span id="line-943">      return;</span>
<span class="source-line-no">944</span><span id="line-944">    }</span>
<span class="source-line-no">945</span><span id="line-945">    conf.set("tmpjars", jars.stream().collect(Collectors.joining(",")));</span>
<span class="source-line-no">946</span><span id="line-946">  }</span>
<span class="source-line-no">947</span><span id="line-947"></span>
<span class="source-line-no">948</span><span id="line-948">  /**</span>
<span class="source-line-no">949</span><span id="line-949">   * Finds the Jar for a class or creates it if it doesn't exist. If the class is in a directory in</span>
<span class="source-line-no">950</span><span id="line-950">   * the classpath, it creates a Jar on the fly with the contents of the directory and returns the</span>
<span class="source-line-no">951</span><span id="line-951">   * path to that Jar. If a Jar is created, it is created in the system temporary directory.</span>
<span class="source-line-no">952</span><span id="line-952">   * Otherwise, returns an existing jar that contains a class of the same name. Maintains a mapping</span>
<span class="source-line-no">953</span><span id="line-953">   * from jar contents to the tmp jar created.</span>
<span class="source-line-no">954</span><span id="line-954">   * @param my_class        the class to find.</span>
<span class="source-line-no">955</span><span id="line-955">   * @param fs              the FileSystem with which to qualify the returned path.</span>
<span class="source-line-no">956</span><span id="line-956">   * @param packagedClasses a map of class name to path.</span>
<span class="source-line-no">957</span><span id="line-957">   * @return a jar file that contains the class.</span>
<span class="source-line-no">958</span><span id="line-958">   */</span>
<span class="source-line-no">959</span><span id="line-959">  private static Path findOrCreateJar(Class&lt;?&gt; my_class, FileSystem fs,</span>
<span class="source-line-no">960</span><span id="line-960">    Map&lt;String, String&gt; packagedClasses) throws IOException {</span>
<span class="source-line-no">961</span><span id="line-961">    // attempt to locate an existing jar for the class.</span>
<span class="source-line-no">962</span><span id="line-962">    String jar = findContainingJar(my_class, packagedClasses);</span>
<span class="source-line-no">963</span><span id="line-963">    if (null == jar || jar.isEmpty()) {</span>
<span class="source-line-no">964</span><span id="line-964">      jar = getJar(my_class);</span>
<span class="source-line-no">965</span><span id="line-965">      updateMap(jar, packagedClasses);</span>
<span class="source-line-no">966</span><span id="line-966">    }</span>
<span class="source-line-no">967</span><span id="line-967"></span>
<span class="source-line-no">968</span><span id="line-968">    if (null == jar || jar.isEmpty()) {</span>
<span class="source-line-no">969</span><span id="line-969">      return null;</span>
<span class="source-line-no">970</span><span id="line-970">    }</span>
<span class="source-line-no">971</span><span id="line-971"></span>
<span class="source-line-no">972</span><span id="line-972">    LOG.debug(String.format("For class %s, using jar %s", my_class.getName(), jar));</span>
<span class="source-line-no">973</span><span id="line-973">    return new Path(jar).makeQualified(fs.getUri(), fs.getWorkingDirectory());</span>
<span class="source-line-no">974</span><span id="line-974">  }</span>
<span class="source-line-no">975</span><span id="line-975"></span>
<span class="source-line-no">976</span><span id="line-976">  /**</span>
<span class="source-line-no">977</span><span id="line-977">   * Add entries to &lt;code&gt;packagedClasses&lt;/code&gt; corresponding to class files contained in</span>
<span class="source-line-no">978</span><span id="line-978">   * &lt;code&gt;jar&lt;/code&gt;.</span>
<span class="source-line-no">979</span><span id="line-979">   * @param jar             The jar who's content to list.</span>
<span class="source-line-no">980</span><span id="line-980">   * @param packagedClasses map[class -&gt; jar]</span>
<span class="source-line-no">981</span><span id="line-981">   */</span>
<span class="source-line-no">982</span><span id="line-982">  private static void updateMap(String jar, Map&lt;String, String&gt; packagedClasses)</span>
<span class="source-line-no">983</span><span id="line-983">    throws IOException {</span>
<span class="source-line-no">984</span><span id="line-984">    if (null == jar || jar.isEmpty()) {</span>
<span class="source-line-no">985</span><span id="line-985">      return;</span>
<span class="source-line-no">986</span><span id="line-986">    }</span>
<span class="source-line-no">987</span><span id="line-987">    ZipFile zip = null;</span>
<span class="source-line-no">988</span><span id="line-988">    try {</span>
<span class="source-line-no">989</span><span id="line-989">      zip = new ZipFile(jar);</span>
<span class="source-line-no">990</span><span id="line-990">      for (Enumeration&lt;? extends ZipEntry&gt; iter = zip.entries(); iter.hasMoreElements();) {</span>
<span class="source-line-no">991</span><span id="line-991">        ZipEntry entry = iter.nextElement();</span>
<span class="source-line-no">992</span><span id="line-992">        if (entry.getName().endsWith("class")) {</span>
<span class="source-line-no">993</span><span id="line-993">          packagedClasses.put(entry.getName(), jar);</span>
<span class="source-line-no">994</span><span id="line-994">        }</span>
<span class="source-line-no">995</span><span id="line-995">      }</span>
<span class="source-line-no">996</span><span id="line-996">    } finally {</span>
<span class="source-line-no">997</span><span id="line-997">      if (null != zip) zip.close();</span>
<span class="source-line-no">998</span><span id="line-998">    }</span>
<span class="source-line-no">999</span><span id="line-999">  }</span>
<span class="source-line-no">1000</span><span id="line-1000"></span>
<span class="source-line-no">1001</span><span id="line-1001">  /**</span>
<span class="source-line-no">1002</span><span id="line-1002">   * Find a jar that contains a class of the same name, if any. It will return a jar file, even if</span>
<span class="source-line-no">1003</span><span id="line-1003">   * that is not the first thing on the class path that has a class with the same name. Looks first</span>
<span class="source-line-no">1004</span><span id="line-1004">   * on the classpath and then in the &lt;code&gt;packagedClasses&lt;/code&gt; map.</span>
<span class="source-line-no">1005</span><span id="line-1005">   * @param my_class the class to find.</span>
<span class="source-line-no">1006</span><span id="line-1006">   * @return a jar file that contains the class, or null.</span>
<span class="source-line-no">1007</span><span id="line-1007">   */</span>
<span class="source-line-no">1008</span><span id="line-1008">  private static String findContainingJar(Class&lt;?&gt; my_class, Map&lt;String, String&gt; packagedClasses)</span>
<span class="source-line-no">1009</span><span id="line-1009">    throws IOException {</span>
<span class="source-line-no">1010</span><span id="line-1010">    ClassLoader loader = my_class.getClassLoader();</span>
<span class="source-line-no">1011</span><span id="line-1011"></span>
<span class="source-line-no">1012</span><span id="line-1012">    String class_file = my_class.getName().replaceAll("\\.", "/") + ".class";</span>
<span class="source-line-no">1013</span><span id="line-1013"></span>
<span class="source-line-no">1014</span><span id="line-1014">    if (loader != null) {</span>
<span class="source-line-no">1015</span><span id="line-1015">      // first search the classpath</span>
<span class="source-line-no">1016</span><span id="line-1016">      for (Enumeration&lt;URL&gt; itr = loader.getResources(class_file); itr.hasMoreElements();) {</span>
<span class="source-line-no">1017</span><span id="line-1017">        URL url = itr.nextElement();</span>
<span class="source-line-no">1018</span><span id="line-1018">        if ("jar".equals(url.getProtocol())) {</span>
<span class="source-line-no">1019</span><span id="line-1019">          String toReturn = url.getPath();</span>
<span class="source-line-no">1020</span><span id="line-1020">          if (toReturn.startsWith("file:")) {</span>
<span class="source-line-no">1021</span><span id="line-1021">            toReturn = toReturn.substring("file:".length());</span>
<span class="source-line-no">1022</span><span id="line-1022">          }</span>
<span class="source-line-no">1023</span><span id="line-1023">          // URLDecoder is a misnamed class, since it actually decodes</span>
<span class="source-line-no">1024</span><span id="line-1024">          // x-www-form-urlencoded MIME type rather than actual</span>
<span class="source-line-no">1025</span><span id="line-1025">          // URL encoding (which the file path has). Therefore it would</span>
<span class="source-line-no">1026</span><span id="line-1026">          // decode +s to ' 's which is incorrect (spaces are actually</span>
<span class="source-line-no">1027</span><span id="line-1027">          // either unencoded or encoded as "%20"). Replace +s first, so</span>
<span class="source-line-no">1028</span><span id="line-1028">          // that they are kept sacred during the decoding process.</span>
<span class="source-line-no">1029</span><span id="line-1029">          toReturn = toReturn.replaceAll("\\+", "%2B");</span>
<span class="source-line-no">1030</span><span id="line-1030">          toReturn = URLDecoder.decode(toReturn, "UTF-8");</span>
<span class="source-line-no">1031</span><span id="line-1031">          return toReturn.replaceAll("!.*$", "");</span>
<span class="source-line-no">1032</span><span id="line-1032">        }</span>
<span class="source-line-no">1033</span><span id="line-1033">      }</span>
<span class="source-line-no">1034</span><span id="line-1034">    }</span>
<span class="source-line-no">1035</span><span id="line-1035"></span>
<span class="source-line-no">1036</span><span id="line-1036">    // now look in any jars we've packaged using JarFinder. Returns null when</span>
<span class="source-line-no">1037</span><span id="line-1037">    // no jar is found.</span>
<span class="source-line-no">1038</span><span id="line-1038">    return packagedClasses.get(class_file);</span>
<span class="source-line-no">1039</span><span id="line-1039">  }</span>
<span class="source-line-no">1040</span><span id="line-1040"></span>
<span class="source-line-no">1041</span><span id="line-1041">  /**</span>
<span class="source-line-no">1042</span><span id="line-1042">   * Invoke 'getJar' on a custom JarFinder implementation. Useful for some job configuration</span>
<span class="source-line-no">1043</span><span id="line-1043">   * contexts (HBASE-8140) and also for testing on MRv2. check if we have HADOOP-9426.</span>
<span class="source-line-no">1044</span><span id="line-1044">   * @param my_class the class to find.</span>
<span class="source-line-no">1045</span><span id="line-1045">   * @return a jar file that contains the class, or null.</span>
<span class="source-line-no">1046</span><span id="line-1046">   */</span>
<span class="source-line-no">1047</span><span id="line-1047">  private static String getJar(Class&lt;?&gt; my_class) {</span>
<span class="source-line-no">1048</span><span id="line-1048">    String ret = null;</span>
<span class="source-line-no">1049</span><span id="line-1049">    try {</span>
<span class="source-line-no">1050</span><span id="line-1050">      ret = JarFinder.getJar(my_class);</span>
<span class="source-line-no">1051</span><span id="line-1051">    } catch (Exception e) {</span>
<span class="source-line-no">1052</span><span id="line-1052">      // toss all other exceptions, related to reflection failure</span>
<span class="source-line-no">1053</span><span id="line-1053">      throw new RuntimeException("getJar invocation failed.", e);</span>
<span class="source-line-no">1054</span><span id="line-1054">    }</span>
<span class="source-line-no">1055</span><span id="line-1055"></span>
<span class="source-line-no">1056</span><span id="line-1056">    return ret;</span>
<span class="source-line-no">1057</span><span id="line-1057">  }</span>
<span class="source-line-no">1058</span><span id="line-1058"></span>
<span class="source-line-no">1059</span><span id="line-1059">  private static int getRegionCount(Configuration conf, TableName tableName) throws IOException {</span>
<span class="source-line-no">1060</span><span id="line-1060">    try (Connection conn = ConnectionFactory.createConnection(conf);</span>
<span class="source-line-no">1061</span><span id="line-1061">      RegionLocator locator = conn.getRegionLocator(tableName)) {</span>
<span class="source-line-no">1062</span><span id="line-1062">      return locator.getAllRegionLocations().size();</span>
<span class="source-line-no">1063</span><span id="line-1063">    }</span>
<span class="source-line-no">1064</span><span id="line-1064">  }</span>
<span class="source-line-no">1065</span><span id="line-1065">}</span>




























































</pre>
</div>
</main>
</body>
</html>
